{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 1 - Tic Tac Toe\n",
    "\n",
    "In this lab you will perform deep learning analysis on a dataset of playing [Tic Tac Toe](https://en.wikipedia.org/wiki/Tic-tac-toe).\n",
    "\n",
    "There are 9 grids in Tic Tac Toe that are coded as the following picture shows:\n",
    "\n",
    "![Tic Tac Toe Grids](tttboard.jpg)\n",
    "\n",
    "In the first 9 columns of the dataset you can find which marks (`x` or `o`) exist in the grids. If there is no mark in a certain grid, it is labeled as `b`. The last column is `class` which tells you whether Player X (who always moves first in Tic Tac Toe) wins in this configuration. Note that when `class` has the value `False`, it means either Player O wins the game or it ends up as a draw."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Follow the steps suggested below to conduct a neural network analysis using Tensorflow and Keras. You will build a deep learning model to predict whether Player X wins the game or not.\n",
    "\n",
    "## Step 1: Data Engineering\n",
    "\n",
    "This dataset is almost in the ready-to-use state so you do not need to worry about missing values and so on. Still, some simple data engineering is needed.\n",
    "\n",
    "1. Read `tic-tac-toe.csv` into a dataframe.\n",
    "1. Inspect the dataset. Determine if the dataset is reliable by eyeballing the data.\n",
    "1. Convert the categorical values to numeric in all columns.\n",
    "1. Separate the inputs and output.\n",
    "1. Normalize the input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"tic-tac-toe.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TL</th>\n",
       "      <th>TM</th>\n",
       "      <th>TR</th>\n",
       "      <th>ML</th>\n",
       "      <th>MM</th>\n",
       "      <th>MR</th>\n",
       "      <th>BL</th>\n",
       "      <th>BM</th>\n",
       "      <th>BR</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>x</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>x</td>\n",
       "      <td>o</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>x</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>b</td>\n",
       "      <td>b</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>x</td>\n",
       "      <td>o</td>\n",
       "      <td>o</td>\n",
       "      <td>b</td>\n",
       "      <td>o</td>\n",
       "      <td>b</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  TL TM TR ML MM MR BL BM BR  class\n",
       "0  x  x  x  x  o  o  x  o  o   True\n",
       "1  x  x  x  x  o  o  o  x  o   True\n",
       "2  x  x  x  x  o  o  o  o  x   True\n",
       "3  x  x  x  x  o  o  o  b  b   True\n",
       "4  x  x  x  x  o  o  b  o  b   True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 958 entries, 0 to 957\n",
      "Data columns (total 10 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   TL      958 non-null    object\n",
      " 1   TM      958 non-null    object\n",
      " 2   TR      958 non-null    object\n",
      " 3   ML      958 non-null    object\n",
      " 4   MM      958 non-null    object\n",
      " 5   MR      958 non-null    object\n",
      " 6   BL      958 non-null    object\n",
      " 7   BM      958 non-null    object\n",
      " 8   BR      958 non-null    object\n",
      " 9   class   958 non-null    bool  \n",
      "dtypes: bool(1), object(9)\n",
      "memory usage: 68.4+ KB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target variable\n",
    "#X = df.drop('class', axis=1)  # Features\n",
    "#y = df['class']               # Target\n",
    "\n",
    "# One-hot encode the features\n",
    "#X_encoded = pd.get_dummies(X, columns=X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_dict = {'x': 1, 'o': 0, 'b': 2}\n",
    "df.replace(mapping_dict, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"class\"] = df[\"class\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      1\n",
       "1      1\n",
       "2      1\n",
       "3      1\n",
       "4      1\n",
       "      ..\n",
       "953    0\n",
       "954    0\n",
       "955    0\n",
       "956    0\n",
       "957    0\n",
       "Name: class, Length: 958, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"class\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      1\n",
       "1      1\n",
       "2      1\n",
       "3      1\n",
       "4      1\n",
       "      ..\n",
       "953    0\n",
       "954    0\n",
       "955    0\n",
       "956    0\n",
       "957    0\n",
       "Name: class, Length: 958, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## X\n",
    "X = df.iloc[:, :9]\n",
    "y = df.iloc[:,-1]\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Build Neural Network\n",
    "\n",
    "To build the neural network, you can refer to your own codes you wrote while following the [Deep Learning with Python, TensorFlow, and Keras tutorial](https://www.youtube.com/watch?v=wQ8BIBpya2k) in the lesson. It's pretty similar to what you will be doing in this lab.\n",
    "\n",
    "1. Split the training and test data.\n",
    "1. Create a `Sequential` model.\n",
    "1. Add several layers to your model. Make sure you use ReLU as the activation function for the middle layers. Use Softmax for the output layer because each output has a single lable and all the label probabilities add up to 1.\n",
    "1. Compile the model using `adam` as the optimizer and `sparse_categorical_crossentropy` as the loss function. For metrics, use `accuracy` for now.\n",
    "1. Fit the training data.\n",
    "1. Evaluate your neural network model with the test data.\n",
    "1. Save your model as `tic-tac-toe.model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-30 17:15:30.914863: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,280</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">130</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │         \u001b[38;5;34m1,280\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m8,256\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)              │           \u001b[38;5;34m130\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">9,666</span> (37.76 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m9,666\u001b[0m (37.76 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">9,666</span> (37.76 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m9,666\u001b[0m (37.76 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Build model\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(128, activation='relu', input_shape=(X_train.shape[1],)))\n",
    "## dropout to improve model\n",
    "model.add(Dropout(0.2))  ## avoid overfitting\n",
    "model.add(Dense(64, activation='relu'))\n",
    "\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/keras/src/optimizers/base_optimizer.py:33: UserWarning: Argument `decay` is no longer supported and will be ignored.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#from keras.optimizers import Adam\n",
    "import tensorflow.keras\n",
    "my_opt = tensorflow.keras.optimizers.Adagrad(learning_rate=0.01, epsilon=0.1, decay=0.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile the model using adam\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer= my_opt, metrics= ['accuracy'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.6236 - loss: 0.6569 - val_accuracy: 0.6510 - val_loss: 0.6093\n",
      "Epoch 2/100\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6663 - loss: 0.6361 - val_accuracy: 0.6510 - val_loss: 0.5980\n",
      "Epoch 3/100\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6823 - loss: 0.5973 - val_accuracy: 0.6562 - val_loss: 0.5907\n",
      "Epoch 4/100\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6870 - loss: 0.5961 - val_accuracy: 0.6562 - val_loss: 0.5840\n",
      "Epoch 5/100\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6691 - loss: 0.5933 - val_accuracy: 0.6719 - val_loss: 0.5799\n",
      "Epoch 6/100\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7041 - loss: 0.5760 - val_accuracy: 0.6510 - val_loss: 0.5726\n",
      "Epoch 7/100\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6648 - loss: 0.6159 - val_accuracy: 0.6771 - val_loss: 0.5780\n",
      "Epoch 8/100\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.6976 - loss: 0.5633 - val_accuracy: 0.6562 - val_loss: 0.5619\n",
      "Epoch 9/100\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.6862 - loss: 0.6010 - val_accuracy: 0.6875 - val_loss: 0.5613\n",
      "Epoch 10/100\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7190 - loss: 0.5788 - val_accuracy: 0.6562 - val_loss: 0.5560\n",
      "Epoch 11/100\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7038 - loss: 0.5657 - val_accuracy: 0.6667 - val_loss: 0.5490\n",
      "Epoch 12/100\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6943 - loss: 0.5871 - val_accuracy: 0.6875 - val_loss: 0.5463\n",
      "Epoch 13/100\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7212 - loss: 0.5538 - val_accuracy: 0.6510 - val_loss: 0.5421\n",
      "Epoch 14/100\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6978 - loss: 0.5580 - val_accuracy: 0.6667 - val_loss: 0.5380\n",
      "Epoch 15/100\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7103 - loss: 0.5366 - val_accuracy: 0.6510 - val_loss: 0.5372\n",
      "Epoch 16/100\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7237 - loss: 0.5304 - val_accuracy: 0.6979 - val_loss: 0.5356\n",
      "Epoch 17/100\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7403 - loss: 0.5254 - val_accuracy: 0.7083 - val_loss: 0.5398\n",
      "Epoch 18/100\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7269 - loss: 0.5343 - val_accuracy: 0.6615 - val_loss: 0.5266\n",
      "Epoch 19/100\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7303 - loss: 0.5582 - val_accuracy: 0.7240 - val_loss: 0.5296\n",
      "Epoch 20/100\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7424 - loss: 0.5341 - val_accuracy: 0.7292 - val_loss: 0.5276\n",
      "Epoch 21/100\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7160 - loss: 0.5443 - val_accuracy: 0.6719 - val_loss: 0.5140\n",
      "Epoch 22/100\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7398 - loss: 0.5386 - val_accuracy: 0.6615 - val_loss: 0.5152\n",
      "Epoch 23/100\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7516 - loss: 0.5014 - val_accuracy: 0.7083 - val_loss: 0.5021\n",
      "Epoch 24/100\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7547 - loss: 0.5025 - val_accuracy: 0.6823 - val_loss: 0.4968\n",
      "Epoch 25/100\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7123 - loss: 0.5444 - val_accuracy: 0.6719 - val_loss: 0.4955\n",
      "Epoch 26/100\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7614 - loss: 0.5054 - val_accuracy: 0.7135 - val_loss: 0.4902\n",
      "Epoch 27/100\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7444 - loss: 0.5026 - val_accuracy: 0.7500 - val_loss: 0.4879\n",
      "Epoch 28/100\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7858 - loss: 0.4765 - val_accuracy: 0.7292 - val_loss: 0.4819\n",
      "Epoch 29/100\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7586 - loss: 0.5053 - val_accuracy: 0.6719 - val_loss: 0.4824\n",
      "Epoch 30/100\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7614 - loss: 0.4950 - val_accuracy: 0.6719 - val_loss: 0.4787\n",
      "Epoch 31/100\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7704 - loss: 0.4523 - val_accuracy: 0.7500 - val_loss: 0.4723\n",
      "Epoch 32/100\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7500 - loss: 0.4929 - val_accuracy: 0.7448 - val_loss: 0.4708\n",
      "Epoch 33/100\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7748 - loss: 0.4879 - val_accuracy: 0.7396 - val_loss: 0.4641\n",
      "Epoch 34/100\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8081 - loss: 0.4573 - val_accuracy: 0.7344 - val_loss: 0.4590\n",
      "Epoch 35/100\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7725 - loss: 0.4627 - val_accuracy: 0.7135 - val_loss: 0.4537\n",
      "Epoch 36/100\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8043 - loss: 0.4364 - val_accuracy: 0.7552 - val_loss: 0.4494\n",
      "Epoch 37/100\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7851 - loss: 0.4568 - val_accuracy: 0.7656 - val_loss: 0.4474\n",
      "Epoch 38/100\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8048 - loss: 0.4544 - val_accuracy: 0.6979 - val_loss: 0.4472\n",
      "Epoch 39/100\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7896 - loss: 0.4703 - val_accuracy: 0.7917 - val_loss: 0.4539\n",
      "Epoch 40/100\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7802 - loss: 0.4506 - val_accuracy: 0.7240 - val_loss: 0.4421\n",
      "Epoch 41/100\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8018 - loss: 0.4426 - val_accuracy: 0.7656 - val_loss: 0.4379\n",
      "Epoch 42/100\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8208 - loss: 0.4265 - val_accuracy: 0.7865 - val_loss: 0.4336\n",
      "Epoch 43/100\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7931 - loss: 0.4377 - val_accuracy: 0.7708 - val_loss: 0.4324\n",
      "Epoch 44/100\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8202 - loss: 0.4176 - val_accuracy: 0.8125 - val_loss: 0.4343\n",
      "Epoch 45/100\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7947 - loss: 0.4356 - val_accuracy: 0.8177 - val_loss: 0.4242\n",
      "Epoch 46/100\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7851 - loss: 0.4396 - val_accuracy: 0.7500 - val_loss: 0.4201\n",
      "Epoch 47/100\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8157 - loss: 0.4336 - val_accuracy: 0.7292 - val_loss: 0.4224\n",
      "Epoch 48/100\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8035 - loss: 0.4355 - val_accuracy: 0.7656 - val_loss: 0.4118\n",
      "Epoch 49/100\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8139 - loss: 0.4106 - val_accuracy: 0.7708 - val_loss: 0.4086\n",
      "Epoch 50/100\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8153 - loss: 0.4110 - val_accuracy: 0.7917 - val_loss: 0.4037\n",
      "Epoch 51/100\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8113 - loss: 0.4030 - val_accuracy: 0.8073 - val_loss: 0.3979\n",
      "Epoch 52/100\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8420 - loss: 0.3910 - val_accuracy: 0.7656 - val_loss: 0.4020\n",
      "Epoch 53/100\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8111 - loss: 0.3893 - val_accuracy: 0.8542 - val_loss: 0.3984\n",
      "Epoch 54/100\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7964 - loss: 0.4220 - val_accuracy: 0.8021 - val_loss: 0.3892\n",
      "Epoch 55/100\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8379 - loss: 0.3759 - val_accuracy: 0.8073 - val_loss: 0.3863\n",
      "Epoch 56/100\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8503 - loss: 0.3780 - val_accuracy: 0.8125 - val_loss: 0.3830\n",
      "Epoch 57/100\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8433 - loss: 0.3659 - val_accuracy: 0.8073 - val_loss: 0.3821\n",
      "Epoch 58/100\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8548 - loss: 0.3474 - val_accuracy: 0.8490 - val_loss: 0.3798\n",
      "Epoch 59/100\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8198 - loss: 0.3991 - val_accuracy: 0.7969 - val_loss: 0.3788\n",
      "Epoch 60/100\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8103 - loss: 0.4014 - val_accuracy: 0.8021 - val_loss: 0.3769\n",
      "Epoch 61/100\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8266 - loss: 0.3680 - val_accuracy: 0.8229 - val_loss: 0.3699\n",
      "Epoch 62/100\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8452 - loss: 0.3509 - val_accuracy: 0.7708 - val_loss: 0.3806\n",
      "Epoch 63/100\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8293 - loss: 0.3813 - val_accuracy: 0.8021 - val_loss: 0.3713\n",
      "Epoch 64/100\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8440 - loss: 0.3527 - val_accuracy: 0.8438 - val_loss: 0.3608\n",
      "Epoch 65/100\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8416 - loss: 0.3802 - val_accuracy: 0.8073 - val_loss: 0.3637\n",
      "Epoch 66/100\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8514 - loss: 0.3426 - val_accuracy: 0.8438 - val_loss: 0.3569\n",
      "Epoch 67/100\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8551 - loss: 0.3638 - val_accuracy: 0.8229 - val_loss: 0.3619\n",
      "Epoch 68/100\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8572 - loss: 0.3495 - val_accuracy: 0.8333 - val_loss: 0.3547\n",
      "Epoch 69/100\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8208 - loss: 0.3891 - val_accuracy: 0.8646 - val_loss: 0.3503\n",
      "Epoch 70/100\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8491 - loss: 0.3715 - val_accuracy: 0.8698 - val_loss: 0.3472\n",
      "Epoch 71/100\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8120 - loss: 0.3984 - val_accuracy: 0.8542 - val_loss: 0.3460\n",
      "Epoch 72/100\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8536 - loss: 0.3482 - val_accuracy: 0.8438 - val_loss: 0.3451\n",
      "Epoch 73/100\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8286 - loss: 0.3605 - val_accuracy: 0.8646 - val_loss: 0.3413\n",
      "Epoch 74/100\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8601 - loss: 0.3594 - val_accuracy: 0.8542 - val_loss: 0.3415\n",
      "Epoch 75/100\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8659 - loss: 0.3365 - val_accuracy: 0.8385 - val_loss: 0.3434\n",
      "Epoch 76/100\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8612 - loss: 0.3433 - val_accuracy: 0.8750 - val_loss: 0.3511\n",
      "Epoch 77/100\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8638 - loss: 0.3408 - val_accuracy: 0.8281 - val_loss: 0.3503\n",
      "Epoch 78/100\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8451 - loss: 0.3611 - val_accuracy: 0.8490 - val_loss: 0.3296\n",
      "Epoch 79/100\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8265 - loss: 0.3693 - val_accuracy: 0.8542 - val_loss: 0.3283\n",
      "Epoch 80/100\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8697 - loss: 0.3409 - val_accuracy: 0.8698 - val_loss: 0.3289\n",
      "Epoch 81/100\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8536 - loss: 0.3648 - val_accuracy: 0.8750 - val_loss: 0.3238\n",
      "Epoch 82/100\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8745 - loss: 0.3230 - val_accuracy: 0.8490 - val_loss: 0.3256\n",
      "Epoch 83/100\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8600 - loss: 0.3426 - val_accuracy: 0.8698 - val_loss: 0.3196\n",
      "Epoch 84/100\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8748 - loss: 0.3134 - val_accuracy: 0.8490 - val_loss: 0.3247\n",
      "Epoch 85/100\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8904 - loss: 0.3202 - val_accuracy: 0.8698 - val_loss: 0.3181\n",
      "Epoch 86/100\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8567 - loss: 0.3383 - val_accuracy: 0.8594 - val_loss: 0.3168\n",
      "Epoch 87/100\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8605 - loss: 0.3332 - val_accuracy: 0.8698 - val_loss: 0.3155\n",
      "Epoch 88/100\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8613 - loss: 0.3275 - val_accuracy: 0.8698 - val_loss: 0.3155\n",
      "Epoch 89/100\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.9098 - loss: 0.2668 - val_accuracy: 0.8854 - val_loss: 0.3120\n",
      "Epoch 90/100\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8449 - loss: 0.3418 - val_accuracy: 0.8802 - val_loss: 0.3107\n",
      "Epoch 91/100\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8552 - loss: 0.3322 - val_accuracy: 0.8854 - val_loss: 0.3094\n",
      "Epoch 92/100\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8662 - loss: 0.3126 - val_accuracy: 0.8750 - val_loss: 0.3105\n",
      "Epoch 93/100\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8539 - loss: 0.3218 - val_accuracy: 0.8854 - val_loss: 0.3038\n",
      "Epoch 94/100\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8785 - loss: 0.2921 - val_accuracy: 0.8750 - val_loss: 0.3045\n",
      "Epoch 95/100\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8988 - loss: 0.3000 - val_accuracy: 0.8854 - val_loss: 0.3031\n",
      "Epoch 96/100\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8984 - loss: 0.2740 - val_accuracy: 0.8854 - val_loss: 0.3020\n",
      "Epoch 97/100\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8800 - loss: 0.2905 - val_accuracy: 0.8854 - val_loss: 0.2985\n",
      "Epoch 98/100\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8617 - loss: 0.3161 - val_accuracy: 0.8958 - val_loss: 0.2982\n",
      "Epoch 99/100\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8850 - loss: 0.3004 - val_accuracy: 0.8958 - val_loss: 0.2938\n",
      "Epoch 100/100\n",
      "\u001b[1m52/52\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.8821 - loss: 0.2865 - val_accuracy: 0.8906 - val_loss: 0.2920\n"
     ]
    }
   ],
   "source": [
    "## fit the training data and evaluate with test data\n",
    "\n",
    "batch_size= 15\n",
    "epochs = 100\n",
    "\n",
    "history = model.fit(X_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(X_test, y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the entire model to a file with .keras extension\n",
    "model.save('tic_tac_toe_model.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Make Predictions\n",
    "\n",
    "Now load your saved model and use it to make predictions on a few random rows in the test dataset. Check if the predictions are correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the model\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "loaded_model = load_model('tic_tac_toe_model.keras') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 5\n",
    "X_sample = X_test[:num_samples]\n",
    "y_sample = y_test[:num_samples]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.66274077, 0.33725923],\n",
       "       [0.02154894, 0.978451  ],\n",
       "       [0.6334786 , 0.3665214 ],\n",
       "       [0.6411602 , 0.35883978],\n",
       "       [0.61231536, 0.3876846 ]], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Make predictions\n",
    "predictions = loaded_model.predict(X_sample)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "836    0\n",
       "477    1\n",
       "350    1\n",
       "891    0\n",
       "855    0\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Improve Your Model\n",
    "\n",
    "Did your model achieve low loss (<0.1) and high accuracy (>0.95)? If not, try to improve your model.\n",
    "\n",
    "But how? There are so many things you can play with in Tensorflow and in the next challenge you'll learn about these things. But in this challenge, let's just do a few things to see if they will help.\n",
    "\n",
    "* Add more layers to your model. If the data are complex you need more layers. But don't use more layers than you need. If adding more layers does not improve the model performance you don't need additional layers.\n",
    "* Adjust the learning rate when you compile the model. This means you will create a custom `tf.keras.optimizers.Adam` instance where you specify the learning rate you want. Then pass the instance to `model.compile` as the optimizer.\n",
    "    * `tf.keras.optimizers.Adam` [reference](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers/Adam).\n",
    "    * Don't worry if you don't understand what the learning rate does. You'll learn about it in the next challenge.\n",
    "* Adjust the number of epochs when you fit the training data to the model. Your model performance continues to improve as you train more epochs. But eventually it will reach the ceiling and the performance will stay the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,280</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">66</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_7 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │         \u001b[38;5;34m1,280\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_5 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_8 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m8,256\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_6 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_9 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m2,080\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_7 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_10 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)              │            \u001b[38;5;34m66\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">11,682</span> (45.63 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m11,682\u001b[0m (45.63 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">11,682</span> (45.63 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m11,682\u001b[0m (45.63 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/keras/src/optimizers/base_optimizer.py:33: UserWarning: Argument `decay` is no longer supported and will be ignored.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/80\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 9ms/step - accuracy: 0.5984 - loss: 0.6636 - val_accuracy: 0.6510 - val_loss: 0.6149\n",
      "Epoch 2/80\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.6552 - loss: 0.6332 - val_accuracy: 0.6562 - val_loss: 0.5927\n",
      "Epoch 3/80\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.6546 - loss: 0.6157 - val_accuracy: 0.6510 - val_loss: 0.5807\n",
      "Epoch 4/80\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.6877 - loss: 0.5885 - val_accuracy: 0.6667 - val_loss: 0.5624\n",
      "Epoch 5/80\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.6625 - loss: 0.6025 - val_accuracy: 0.7135 - val_loss: 0.5430\n",
      "Epoch 6/80\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.6477 - loss: 0.6099 - val_accuracy: 0.6615 - val_loss: 0.5347\n",
      "Epoch 7/80\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7081 - loss: 0.5834 - val_accuracy: 0.7344 - val_loss: 0.5094\n",
      "Epoch 8/80\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7142 - loss: 0.5625 - val_accuracy: 0.7604 - val_loss: 0.4970\n",
      "Epoch 9/80\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7035 - loss: 0.5314 - val_accuracy: 0.7396 - val_loss: 0.4802\n",
      "Epoch 10/80\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7288 - loss: 0.5297 - val_accuracy: 0.7708 - val_loss: 0.4564\n",
      "Epoch 11/80\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7509 - loss: 0.4858 - val_accuracy: 0.7812 - val_loss: 0.4261\n",
      "Epoch 12/80\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7745 - loss: 0.4700 - val_accuracy: 0.7760 - val_loss: 0.4394\n",
      "Epoch 13/80\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7759 - loss: 0.4638 - val_accuracy: 0.7917 - val_loss: 0.4092\n",
      "Epoch 14/80\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7939 - loss: 0.4315 - val_accuracy: 0.7812 - val_loss: 0.3869\n",
      "Epoch 15/80\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8212 - loss: 0.3924 - val_accuracy: 0.8125 - val_loss: 0.4033\n",
      "Epoch 16/80\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8468 - loss: 0.3604 - val_accuracy: 0.8229 - val_loss: 0.3580\n",
      "Epoch 17/80\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8480 - loss: 0.3417 - val_accuracy: 0.8281 - val_loss: 0.4041\n",
      "Epoch 18/80\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8576 - loss: 0.3284 - val_accuracy: 0.8594 - val_loss: 0.3233\n",
      "Epoch 19/80\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8620 - loss: 0.3074 - val_accuracy: 0.8750 - val_loss: 0.2869\n",
      "Epoch 20/80\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8847 - loss: 0.2912 - val_accuracy: 0.8594 - val_loss: 0.2662\n",
      "Epoch 21/80\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9196 - loss: 0.2450 - val_accuracy: 0.9167 - val_loss: 0.2345\n",
      "Epoch 22/80\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.8995 - loss: 0.2785 - val_accuracy: 0.9115 - val_loss: 0.2325\n",
      "Epoch 23/80\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.8983 - loss: 0.2538 - val_accuracy: 0.9323 - val_loss: 0.2020\n",
      "Epoch 24/80\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9196 - loss: 0.1950 - val_accuracy: 0.9583 - val_loss: 0.1746\n",
      "Epoch 25/80\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9437 - loss: 0.1948 - val_accuracy: 0.9583 - val_loss: 0.1556\n",
      "Epoch 26/80\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9328 - loss: 0.1833 - val_accuracy: 0.9219 - val_loss: 0.2003\n",
      "Epoch 27/80\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9321 - loss: 0.1683 - val_accuracy: 0.9427 - val_loss: 0.1568\n",
      "Epoch 28/80\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9377 - loss: 0.1673 - val_accuracy: 0.9271 - val_loss: 0.1711\n",
      "Epoch 29/80\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.9554 - loss: 0.1388 - val_accuracy: 0.9271 - val_loss: 0.1928\n",
      "Epoch 30/80\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9650 - loss: 0.1166 - val_accuracy: 0.9583 - val_loss: 0.1291\n",
      "Epoch 31/80\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 12ms/step - accuracy: 0.9599 - loss: 0.1213 - val_accuracy: 0.9688 - val_loss: 0.1200\n",
      "Epoch 32/80\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 16ms/step - accuracy: 0.9699 - loss: 0.0976 - val_accuracy: 0.9375 - val_loss: 0.1429\n",
      "Epoch 33/80\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.9584 - loss: 0.1368 - val_accuracy: 0.9583 - val_loss: 0.1256\n",
      "Epoch 34/80\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 19ms/step - accuracy: 0.9714 - loss: 0.0746 - val_accuracy: 0.9479 - val_loss: 0.1503\n",
      "Epoch 35/80\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9716 - loss: 0.0933 - val_accuracy: 0.9635 - val_loss: 0.1214\n",
      "Epoch 36/80\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9608 - loss: 0.1094 - val_accuracy: 0.9688 - val_loss: 0.1077\n",
      "Epoch 37/80\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - accuracy: 0.9781 - loss: 0.0639 - val_accuracy: 0.9583 - val_loss: 0.1079\n",
      "Epoch 38/80\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step - accuracy: 0.9465 - loss: 0.1268 - val_accuracy: 0.9688 - val_loss: 0.1265\n",
      "Epoch 39/80\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9744 - loss: 0.0898 - val_accuracy: 0.9688 - val_loss: 0.1025\n",
      "Epoch 40/80\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9770 - loss: 0.0735 - val_accuracy: 0.9219 - val_loss: 0.1501\n",
      "Epoch 41/80\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9452 - loss: 0.1542 - val_accuracy: 0.9583 - val_loss: 0.1230\n",
      "Epoch 42/80\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9580 - loss: 0.1131 - val_accuracy: 0.9688 - val_loss: 0.0996\n",
      "Epoch 43/80\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.9732 - loss: 0.0951 - val_accuracy: 0.9688 - val_loss: 0.1039\n",
      "Epoch 44/80\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.9751 - loss: 0.0872 - val_accuracy: 0.9635 - val_loss: 0.1086\n",
      "Epoch 45/80\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9887 - loss: 0.0521 - val_accuracy: 0.9531 - val_loss: 0.1085\n",
      "Epoch 46/80\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9819 - loss: 0.0497 - val_accuracy: 0.9635 - val_loss: 0.0968\n",
      "Epoch 47/80\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9702 - loss: 0.0741 - val_accuracy: 0.9635 - val_loss: 0.0996\n",
      "Epoch 48/80\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9657 - loss: 0.0932 - val_accuracy: 0.9688 - val_loss: 0.0940\n",
      "Epoch 49/80\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9717 - loss: 0.0704 - val_accuracy: 0.9688 - val_loss: 0.0912\n",
      "Epoch 50/80\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9722 - loss: 0.0650 - val_accuracy: 0.9583 - val_loss: 0.1122\n",
      "Epoch 51/80\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9839 - loss: 0.0545 - val_accuracy: 0.9375 - val_loss: 0.1471\n",
      "Epoch 52/80\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - accuracy: 0.9585 - loss: 0.1306 - val_accuracy: 0.9271 - val_loss: 0.2518\n",
      "Epoch 53/80\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9637 - loss: 0.0962 - val_accuracy: 0.9688 - val_loss: 0.0934\n",
      "Epoch 54/80\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9814 - loss: 0.0516 - val_accuracy: 0.9688 - val_loss: 0.0937\n",
      "Epoch 55/80\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9800 - loss: 0.0606 - val_accuracy: 0.9740 - val_loss: 0.0754\n",
      "Epoch 56/80\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9772 - loss: 0.0552 - val_accuracy: 0.9583 - val_loss: 0.1018\n",
      "Epoch 57/80\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9803 - loss: 0.0619 - val_accuracy: 0.9688 - val_loss: 0.0934\n",
      "Epoch 58/80\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9773 - loss: 0.0458 - val_accuracy: 0.9740 - val_loss: 0.0628\n",
      "Epoch 59/80\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9867 - loss: 0.0455 - val_accuracy: 0.9427 - val_loss: 0.1398\n",
      "Epoch 60/80\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9574 - loss: 0.0829 - val_accuracy: 0.9740 - val_loss: 0.0942\n",
      "Epoch 61/80\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9827 - loss: 0.0383 - val_accuracy: 0.9688 - val_loss: 0.1048\n",
      "Epoch 62/80\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.9836 - loss: 0.0497 - val_accuracy: 0.9635 - val_loss: 0.1014\n",
      "Epoch 63/80\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 13ms/step - accuracy: 0.9797 - loss: 0.0687 - val_accuracy: 0.9688 - val_loss: 0.0968\n",
      "Epoch 64/80\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 15ms/step - accuracy: 0.9958 - loss: 0.0233 - val_accuracy: 0.9688 - val_loss: 0.0825\n",
      "Epoch 65/80\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.9818 - loss: 0.0597 - val_accuracy: 0.9688 - val_loss: 0.0763\n",
      "Epoch 66/80\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9763 - loss: 0.0530 - val_accuracy: 0.9583 - val_loss: 0.0741\n",
      "Epoch 67/80\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9867 - loss: 0.0577 - val_accuracy: 0.9688 - val_loss: 0.0961\n",
      "Epoch 68/80\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9900 - loss: 0.0280 - val_accuracy: 0.9688 - val_loss: 0.1117\n",
      "Epoch 69/80\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9884 - loss: 0.0361 - val_accuracy: 0.9688 - val_loss: 0.0846\n",
      "Epoch 70/80\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9758 - loss: 0.0652 - val_accuracy: 0.9688 - val_loss: 0.0999\n",
      "Epoch 71/80\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9804 - loss: 0.0650 - val_accuracy: 0.9688 - val_loss: 0.0939\n",
      "Epoch 72/80\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 15ms/step - accuracy: 0.9694 - loss: 0.0638 - val_accuracy: 0.9688 - val_loss: 0.0673\n",
      "Epoch 73/80\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 11ms/step - accuracy: 0.9828 - loss: 0.0440 - val_accuracy: 0.9688 - val_loss: 0.0832\n",
      "Epoch 74/80\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 13ms/step - accuracy: 0.9884 - loss: 0.0360 - val_accuracy: 0.9688 - val_loss: 0.0733\n",
      "Epoch 75/80\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9850 - loss: 0.0361 - val_accuracy: 0.9740 - val_loss: 0.0660\n",
      "Epoch 76/80\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9943 - loss: 0.0223 - val_accuracy: 0.9688 - val_loss: 0.0747\n",
      "Epoch 77/80\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 9ms/step - accuracy: 0.9944 - loss: 0.0203 - val_accuracy: 0.9688 - val_loss: 0.0567\n",
      "Epoch 78/80\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9903 - loss: 0.0288 - val_accuracy: 0.9688 - val_loss: 0.0568\n",
      "Epoch 79/80\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - accuracy: 0.9959 - loss: 0.0323 - val_accuracy: 0.9688 - val_loss: 0.0999\n",
      "Epoch 80/80\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 10ms/step - accuracy: 0.9953 - loss: 0.0181 - val_accuracy: 0.9635 - val_loss: 0.0758\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "\n",
    "model2 = Sequential()\n",
    "\n",
    "model2.add(Dense(128, activation='relu', input_shape=(X_train.shape[1],)))\n",
    "## dropout to improve model\n",
    "model2.add(Dropout(0.2))  ## avoid overfitting\n",
    "model2.add(Dense(64, activation='relu'))\n",
    "\n",
    "model2.add(Dropout(0.2))  ## avoid overfitting\n",
    "model2.add(Dense(32, activation='relu'))\n",
    "\n",
    "model2.add(Dropout(0.2))\n",
    "model2.add(Dense(2, activation='softmax'))\n",
    "\n",
    "model2.summary()\n",
    "\n",
    "\n",
    "my_opt = tensorflow.keras.optimizers.Adam(learning_rate=0.01, epsilon=0.1, decay=0.0)\n",
    "\n",
    "model2.compile(loss='sparse_categorical_crossentropy', optimizer= my_opt, metrics= ['accuracy'] )\n",
    "\n",
    "\n",
    "batch_size= 10\n",
    "epochs = 80\n",
    "\n",
    "history = model2.fit(X_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.07581420987844467\n",
      "Test accuraccy: 0.9635416865348816\n"
     ]
    }
   ],
   "source": [
    "score = model2.evaluate(X_test, y_test, verbose=3)\n",
    "print(\"Test loss:\", score[0])\n",
    "print(\"Test accuraccy:\", score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.2919815480709076\n",
      "Test accuraccy: 0.890625\n"
     ]
    }
   ],
   "source": [
    "score = model.evaluate(X_test, y_test, verbose=3)\n",
    "print(\"Test loss:\", score[0])\n",
    "print(\"Test accuraccy:\", score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_7\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_7\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_27 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,280</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_20 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_28 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_21 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_29 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_22 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_30 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">66</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_27 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │         \u001b[38;5;34m1,280\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_20 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_28 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m8,256\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_21 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_29 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m2,080\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_22 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_30 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)              │            \u001b[38;5;34m66\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">11,682</span> (45.63 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m11,682\u001b[0m (45.63 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">11,682</span> (45.63 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m11,682\u001b[0m (45.63 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/keras/src/optimizers/base_optimizer.py:33: UserWarning: Argument `decay` is no longer supported and will be ignored.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/70\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 252ms/step - accuracy: 0.5080 - loss: 0.7061 - val_accuracy: 0.6510 - val_loss: 0.6318\n",
      "Epoch 2/70\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - accuracy: 0.6432 - loss: 0.6382 - val_accuracy: 0.6510 - val_loss: 0.6167\n",
      "Epoch 3/70\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.6830 - loss: 0.6050 - val_accuracy: 0.6615 - val_loss: 0.5992\n",
      "Epoch 4/70\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 37ms/step - accuracy: 0.6456 - loss: 0.6103 - val_accuracy: 0.6667 - val_loss: 0.5821\n",
      "Epoch 5/70\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 56ms/step - accuracy: 0.6821 - loss: 0.6005 - val_accuracy: 0.6719 - val_loss: 0.5696\n",
      "Epoch 6/70\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 27ms/step - accuracy: 0.6963 - loss: 0.5727 - val_accuracy: 0.6771 - val_loss: 0.5562\n",
      "Epoch 7/70\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 49ms/step - accuracy: 0.6771 - loss: 0.6144 - val_accuracy: 0.6510 - val_loss: 0.5668\n",
      "Epoch 8/70\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 23ms/step - accuracy: 0.6816 - loss: 0.5900 - val_accuracy: 0.6875 - val_loss: 0.5396\n",
      "Epoch 9/70\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - accuracy: 0.7018 - loss: 0.5625 - val_accuracy: 0.6719 - val_loss: 0.5296\n",
      "Epoch 10/70\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - accuracy: 0.6986 - loss: 0.5541 - val_accuracy: 0.6979 - val_loss: 0.4983\n",
      "Epoch 11/70\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.7233 - loss: 0.5468 - val_accuracy: 0.7240 - val_loss: 0.4937\n",
      "Epoch 12/70\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 65ms/step - accuracy: 0.7225 - loss: 0.5245 - val_accuracy: 0.7188 - val_loss: 0.4910\n",
      "Epoch 13/70\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 57ms/step - accuracy: 0.7370 - loss: 0.5197 - val_accuracy: 0.6927 - val_loss: 0.4830\n",
      "Epoch 14/70\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.7661 - loss: 0.4901 - val_accuracy: 0.7812 - val_loss: 0.4378\n",
      "Epoch 15/70\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.7922 - loss: 0.4532 - val_accuracy: 0.7865 - val_loss: 0.4231\n",
      "Epoch 16/70\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - accuracy: 0.7985 - loss: 0.4328 - val_accuracy: 0.7552 - val_loss: 0.4610\n",
      "Epoch 17/70\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.8025 - loss: 0.4142 - val_accuracy: 0.7917 - val_loss: 0.4232\n",
      "Epoch 18/70\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - accuracy: 0.7992 - loss: 0.4255 - val_accuracy: 0.8073 - val_loss: 0.3888\n",
      "Epoch 19/70\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.8192 - loss: 0.3883 - val_accuracy: 0.8073 - val_loss: 0.3572\n",
      "Epoch 20/70\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.8205 - loss: 0.4047 - val_accuracy: 0.8125 - val_loss: 0.3490\n",
      "Epoch 21/70\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.8595 - loss: 0.3283 - val_accuracy: 0.8229 - val_loss: 0.3696\n",
      "Epoch 22/70\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - accuracy: 0.8450 - loss: 0.3626 - val_accuracy: 0.8594 - val_loss: 0.3406\n",
      "Epoch 23/70\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - accuracy: 0.8491 - loss: 0.3405 - val_accuracy: 0.7708 - val_loss: 0.4538\n",
      "Epoch 24/70\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 39ms/step - accuracy: 0.8408 - loss: 0.3733 - val_accuracy: 0.8333 - val_loss: 0.3526\n",
      "Epoch 25/70\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 36ms/step - accuracy: 0.8883 - loss: 0.2982 - val_accuracy: 0.8802 - val_loss: 0.3062\n",
      "Epoch 26/70\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - accuracy: 0.8864 - loss: 0.2923 - val_accuracy: 0.8906 - val_loss: 0.2807\n",
      "Epoch 27/70\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 46ms/step - accuracy: 0.8991 - loss: 0.2427 - val_accuracy: 0.8854 - val_loss: 0.2883\n",
      "Epoch 28/70\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 0.9022 - loss: 0.2525 - val_accuracy: 0.8854 - val_loss: 0.2762\n",
      "Epoch 29/70\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - accuracy: 0.8966 - loss: 0.2772 - val_accuracy: 0.9271 - val_loss: 0.2158\n",
      "Epoch 30/70\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.9291 - loss: 0.2061 - val_accuracy: 0.9010 - val_loss: 0.2614\n",
      "Epoch 31/70\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - accuracy: 0.9296 - loss: 0.2068 - val_accuracy: 0.8958 - val_loss: 0.2222\n",
      "Epoch 32/70\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 51ms/step - accuracy: 0.9167 - loss: 0.2130 - val_accuracy: 0.9323 - val_loss: 0.1948\n",
      "Epoch 33/70\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 47ms/step - accuracy: 0.9435 - loss: 0.1744 - val_accuracy: 0.9531 - val_loss: 0.1689\n",
      "Epoch 34/70\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 42ms/step - accuracy: 0.9572 - loss: 0.1522 - val_accuracy: 0.9167 - val_loss: 0.2456\n",
      "Epoch 35/70\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - accuracy: 0.9375 - loss: 0.1661 - val_accuracy: 0.9479 - val_loss: 0.1769\n",
      "Epoch 36/70\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 26ms/step - accuracy: 0.9615 - loss: 0.1366 - val_accuracy: 0.9479 - val_loss: 0.1679\n",
      "Epoch 37/70\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 63ms/step - accuracy: 0.9545 - loss: 0.1330 - val_accuracy: 0.9688 - val_loss: 0.1271\n",
      "Epoch 38/70\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 29ms/step - accuracy: 0.9591 - loss: 0.1065 - val_accuracy: 0.9688 - val_loss: 0.1324\n",
      "Epoch 39/70\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - accuracy: 0.9522 - loss: 0.1327 - val_accuracy: 0.9583 - val_loss: 0.1301\n",
      "Epoch 40/70\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 34ms/step - accuracy: 0.9550 - loss: 0.1249 - val_accuracy: 0.9531 - val_loss: 0.1316\n",
      "Epoch 41/70\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 46ms/step - accuracy: 0.9573 - loss: 0.1197 - val_accuracy: 0.9583 - val_loss: 0.1426\n",
      "Epoch 42/70\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 47ms/step - accuracy: 0.9747 - loss: 0.0872 - val_accuracy: 0.9479 - val_loss: 0.1650\n",
      "Epoch 43/70\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9685 - loss: 0.1168 - val_accuracy: 0.9635 - val_loss: 0.1093\n",
      "Epoch 44/70\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 18ms/step - accuracy: 0.9697 - loss: 0.0917 - val_accuracy: 0.9635 - val_loss: 0.1237\n",
      "Epoch 45/70\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 35ms/step - accuracy: 0.9765 - loss: 0.0745 - val_accuracy: 0.9583 - val_loss: 0.1267\n",
      "Epoch 46/70\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - accuracy: 0.9764 - loss: 0.0628 - val_accuracy: 0.9688 - val_loss: 0.1058\n",
      "Epoch 47/70\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 41ms/step - accuracy: 0.9687 - loss: 0.0890 - val_accuracy: 0.9635 - val_loss: 0.1536\n",
      "Epoch 48/70\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 17ms/step - accuracy: 0.9654 - loss: 0.1126 - val_accuracy: 0.9531 - val_loss: 0.1181\n",
      "Epoch 49/70\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - accuracy: 0.9680 - loss: 0.0929 - val_accuracy: 0.9688 - val_loss: 0.1010\n",
      "Epoch 50/70\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.9764 - loss: 0.0693 - val_accuracy: 0.9688 - val_loss: 0.0938\n",
      "Epoch 51/70\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9749 - loss: 0.0646 - val_accuracy: 0.9688 - val_loss: 0.0994\n",
      "Epoch 52/70\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - accuracy: 0.9825 - loss: 0.0470 - val_accuracy: 0.9688 - val_loss: 0.1124\n",
      "Epoch 53/70\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.9638 - loss: 0.0736 - val_accuracy: 0.9688 - val_loss: 0.1016\n",
      "Epoch 54/70\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.9805 - loss: 0.0614 - val_accuracy: 0.9688 - val_loss: 0.0870\n",
      "Epoch 55/70\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 52ms/step - accuracy: 0.9734 - loss: 0.0527 - val_accuracy: 0.9688 - val_loss: 0.1002\n",
      "Epoch 56/70\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 39ms/step - accuracy: 0.9757 - loss: 0.0722 - val_accuracy: 0.9688 - val_loss: 0.1050\n",
      "Epoch 57/70\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 51ms/step - accuracy: 0.9672 - loss: 0.0760 - val_accuracy: 0.9635 - val_loss: 0.1055\n",
      "Epoch 58/70\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 16ms/step - accuracy: 0.9744 - loss: 0.0583 - val_accuracy: 0.9688 - val_loss: 0.0895\n",
      "Epoch 59/70\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.9754 - loss: 0.0584 - val_accuracy: 0.9688 - val_loss: 0.1270\n",
      "Epoch 60/70\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 25ms/step - accuracy: 0.9811 - loss: 0.0786 - val_accuracy: 0.9688 - val_loss: 0.1014\n",
      "Epoch 61/70\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 30ms/step - accuracy: 0.9886 - loss: 0.0453 - val_accuracy: 0.9635 - val_loss: 0.0880\n",
      "Epoch 62/70\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 45ms/step - accuracy: 0.9820 - loss: 0.0549 - val_accuracy: 0.9688 - val_loss: 0.0888\n",
      "Epoch 63/70\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - accuracy: 0.9863 - loss: 0.0424 - val_accuracy: 0.9688 - val_loss: 0.0966\n",
      "Epoch 64/70\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 60ms/step - accuracy: 0.9813 - loss: 0.0530 - val_accuracy: 0.9688 - val_loss: 0.1024\n",
      "Epoch 65/70\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 44ms/step - accuracy: 0.9692 - loss: 0.1225 - val_accuracy: 0.9688 - val_loss: 0.1046\n",
      "Epoch 66/70\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - accuracy: 0.9875 - loss: 0.0478 - val_accuracy: 0.9688 - val_loss: 0.0936\n",
      "Epoch 67/70\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 56ms/step - accuracy: 0.9803 - loss: 0.0566 - val_accuracy: 0.9635 - val_loss: 0.1301\n",
      "Epoch 68/70\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 18ms/step - accuracy: 0.9674 - loss: 0.0859 - val_accuracy: 0.9688 - val_loss: 0.1013\n",
      "Epoch 69/70\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - accuracy: 0.9849 - loss: 0.0502 - val_accuracy: 0.9688 - val_loss: 0.0931\n",
      "Epoch 70/70\n",
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 50ms/step - accuracy: 0.9873 - loss: 0.0440 - val_accuracy: 0.9688 - val_loss: 0.0904\n"
     ]
    }
   ],
   "source": [
    "## ajust the leaarning rate\n",
    "\n",
    "model3 = Sequential()\n",
    "\n",
    "model3.add(Dense(128, activation='relu', input_shape=(X_train.shape[1],)))\n",
    "## dropout to improve model\n",
    "model3.add(Dropout(0.2))  ## avoid overfitting\n",
    "model3.add(Dense(64, activation='relu'))\n",
    "\n",
    "model3.add(Dropout(0.2))  ## avoid overfitting\n",
    "model3.add(Dense(32, activation='relu'))\n",
    "\n",
    "model3.add(Dropout(0.2))\n",
    "model3.add(Dense(2, activation='softmax'))\n",
    "\n",
    "model3.summary()\n",
    "\n",
    "\n",
    "my_opt = tensorflow.keras.optimizers.Adam(learning_rate=0.025, epsilon=0.1, decay=0.0)\n",
    "\n",
    "model3.compile(loss='sparse_categorical_crossentropy', optimizer= my_opt, metrics= ['accuracy'] )\n",
    "\n",
    "\n",
    "batch_size= 32\n",
    "epochs = 70\n",
    "\n",
    "history = model3.fit(X_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_15\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_15\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_59 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,280</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_44 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_60 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">8,256</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_45 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_61 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │         <span style=\"color: #00af00; text-decoration-color: #00af00\">2,080</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_46 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_62 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">66</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_59 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │         \u001b[38;5;34m1,280\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_44 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_60 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │         \u001b[38;5;34m8,256\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_45 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_61 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │         \u001b[38;5;34m2,080\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_46 (\u001b[38;5;33mDropout\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_62 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m)              │            \u001b[38;5;34m66\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">11,682</span> (45.63 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m11,682\u001b[0m (45.63 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">11,682</span> (45.63 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m11,682\u001b[0m (45.63 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.11/site-packages/keras/src/optimizers/base_optimizer.py:33: UserWarning: Argument `decay` is no longer supported and will be ignored.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.6277 - loss: 0.6652 - val_accuracy: 0.6562 - val_loss: 0.6538\n",
      "Epoch 2/150\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6427 - loss: 0.6639 - val_accuracy: 0.6510 - val_loss: 0.6409\n",
      "Epoch 3/150\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6368 - loss: 0.6445 - val_accuracy: 0.6510 - val_loss: 0.6321\n",
      "Epoch 4/150\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6505 - loss: 0.6422 - val_accuracy: 0.6510 - val_loss: 0.6274\n",
      "Epoch 5/150\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6185 - loss: 0.6567 - val_accuracy: 0.6510 - val_loss: 0.6215\n",
      "Epoch 6/150\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6378 - loss: 0.6357 - val_accuracy: 0.6510 - val_loss: 0.6171\n",
      "Epoch 7/150\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6348 - loss: 0.6433 - val_accuracy: 0.6510 - val_loss: 0.6125\n",
      "Epoch 8/150\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6746 - loss: 0.6169 - val_accuracy: 0.6510 - val_loss: 0.6088\n",
      "Epoch 9/150\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6677 - loss: 0.6247 - val_accuracy: 0.6510 - val_loss: 0.6034\n",
      "Epoch 10/150\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6702 - loss: 0.6297 - val_accuracy: 0.6562 - val_loss: 0.6024\n",
      "Epoch 11/150\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6604 - loss: 0.6179 - val_accuracy: 0.6562 - val_loss: 0.5951\n",
      "Epoch 12/150\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6510 - loss: 0.6267 - val_accuracy: 0.6510 - val_loss: 0.5901\n",
      "Epoch 13/150\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6646 - loss: 0.6109 - val_accuracy: 0.6719 - val_loss: 0.5879\n",
      "Epoch 14/150\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6745 - loss: 0.6034 - val_accuracy: 0.6667 - val_loss: 0.5852\n",
      "Epoch 15/150\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6829 - loss: 0.5954 - val_accuracy: 0.6667 - val_loss: 0.5842\n",
      "Epoch 16/150\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6664 - loss: 0.6105 - val_accuracy: 0.6615 - val_loss: 0.5778\n",
      "Epoch 17/150\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6349 - loss: 0.6255 - val_accuracy: 0.6667 - val_loss: 0.5738\n",
      "Epoch 18/150\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6465 - loss: 0.6127 - val_accuracy: 0.6667 - val_loss: 0.5706\n",
      "Epoch 19/150\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6614 - loss: 0.6058 - val_accuracy: 0.6771 - val_loss: 0.5673\n",
      "Epoch 20/150\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6805 - loss: 0.6045 - val_accuracy: 0.6719 - val_loss: 0.5629\n",
      "Epoch 21/150\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6752 - loss: 0.5929 - val_accuracy: 0.6719 - val_loss: 0.5581\n",
      "Epoch 22/150\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6891 - loss: 0.5864 - val_accuracy: 0.6667 - val_loss: 0.5548\n",
      "Epoch 23/150\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6856 - loss: 0.5861 - val_accuracy: 0.6667 - val_loss: 0.5510\n",
      "Epoch 24/150\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6787 - loss: 0.5999 - val_accuracy: 0.6771 - val_loss: 0.5475\n",
      "Epoch 25/150\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6899 - loss: 0.5732 - val_accuracy: 0.6875 - val_loss: 0.5444\n",
      "Epoch 26/150\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6953 - loss: 0.5780 - val_accuracy: 0.6719 - val_loss: 0.5418\n",
      "Epoch 27/150\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6834 - loss: 0.5612 - val_accuracy: 0.6771 - val_loss: 0.5365\n",
      "Epoch 28/150\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6739 - loss: 0.5773 - val_accuracy: 0.6719 - val_loss: 0.5340\n",
      "Epoch 29/150\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7223 - loss: 0.5334 - val_accuracy: 0.6979 - val_loss: 0.5287\n",
      "Epoch 30/150\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7101 - loss: 0.5507 - val_accuracy: 0.6979 - val_loss: 0.5245\n",
      "Epoch 31/150\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7020 - loss: 0.5603 - val_accuracy: 0.6823 - val_loss: 0.5206\n",
      "Epoch 32/150\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6963 - loss: 0.5565 - val_accuracy: 0.6823 - val_loss: 0.5154\n",
      "Epoch 33/150\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6846 - loss: 0.5478 - val_accuracy: 0.7135 - val_loss: 0.5110\n",
      "Epoch 34/150\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7099 - loss: 0.5546 - val_accuracy: 0.6771 - val_loss: 0.5101\n",
      "Epoch 35/150\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7064 - loss: 0.5414 - val_accuracy: 0.6927 - val_loss: 0.5018\n",
      "Epoch 36/150\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6966 - loss: 0.5510 - val_accuracy: 0.7240 - val_loss: 0.4998\n",
      "Epoch 37/150\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7347 - loss: 0.5209 - val_accuracy: 0.7031 - val_loss: 0.4914\n",
      "Epoch 38/150\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7054 - loss: 0.5515 - val_accuracy: 0.7344 - val_loss: 0.4880\n",
      "Epoch 39/150\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7434 - loss: 0.5174 - val_accuracy: 0.7448 - val_loss: 0.4829\n",
      "Epoch 40/150\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7451 - loss: 0.5078 - val_accuracy: 0.7604 - val_loss: 0.4792\n",
      "Epoch 41/150\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7404 - loss: 0.5292 - val_accuracy: 0.7135 - val_loss: 0.4754\n",
      "Epoch 42/150\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7506 - loss: 0.4942 - val_accuracy: 0.7344 - val_loss: 0.4671\n",
      "Epoch 43/150\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7398 - loss: 0.5214 - val_accuracy: 0.7292 - val_loss: 0.4667\n",
      "Epoch 44/150\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7585 - loss: 0.5156 - val_accuracy: 0.7135 - val_loss: 0.4635\n",
      "Epoch 45/150\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7545 - loss: 0.4875 - val_accuracy: 0.7344 - val_loss: 0.4594\n",
      "Epoch 46/150\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7290 - loss: 0.5028 - val_accuracy: 0.7656 - val_loss: 0.4525\n",
      "Epoch 47/150\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7843 - loss: 0.4773 - val_accuracy: 0.7344 - val_loss: 0.4530\n",
      "Epoch 48/150\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7674 - loss: 0.4925 - val_accuracy: 0.7656 - val_loss: 0.4383\n",
      "Epoch 49/150\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7541 - loss: 0.4637 - val_accuracy: 0.7552 - val_loss: 0.4381\n",
      "Epoch 50/150\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7942 - loss: 0.4440 - val_accuracy: 0.7344 - val_loss: 0.4332\n",
      "Epoch 51/150\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7555 - loss: 0.4761 - val_accuracy: 0.7969 - val_loss: 0.4213\n",
      "Epoch 52/150\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7691 - loss: 0.4772 - val_accuracy: 0.8229 - val_loss: 0.4214\n",
      "Epoch 53/150\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7651 - loss: 0.4825 - val_accuracy: 0.7552 - val_loss: 0.4194\n",
      "Epoch 54/150\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7822 - loss: 0.4479 - val_accuracy: 0.7656 - val_loss: 0.4131\n",
      "Epoch 55/150\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8111 - loss: 0.4134 - val_accuracy: 0.7917 - val_loss: 0.4076\n",
      "Epoch 56/150\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7814 - loss: 0.4459 - val_accuracy: 0.8333 - val_loss: 0.4047\n",
      "Epoch 57/150\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8050 - loss: 0.4240 - val_accuracy: 0.8177 - val_loss: 0.3967\n",
      "Epoch 58/150\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8044 - loss: 0.4290 - val_accuracy: 0.8281 - val_loss: 0.3898\n",
      "Epoch 59/150\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7995 - loss: 0.4329 - val_accuracy: 0.8177 - val_loss: 0.3847\n",
      "Epoch 60/150\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7809 - loss: 0.4341 - val_accuracy: 0.8333 - val_loss: 0.3799\n",
      "Epoch 61/150\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7796 - loss: 0.4223 - val_accuracy: 0.8594 - val_loss: 0.3710\n",
      "Epoch 62/150\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7954 - loss: 0.4459 - val_accuracy: 0.8750 - val_loss: 0.3613\n",
      "Epoch 63/150\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7909 - loss: 0.4247 - val_accuracy: 0.8698 - val_loss: 0.3658\n",
      "Epoch 64/150\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8206 - loss: 0.4097 - val_accuracy: 0.8646 - val_loss: 0.3525\n",
      "Epoch 65/150\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8221 - loss: 0.3944 - val_accuracy: 0.8646 - val_loss: 0.3442\n",
      "Epoch 66/150\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8078 - loss: 0.4127 - val_accuracy: 0.8646 - val_loss: 0.3470\n",
      "Epoch 67/150\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8337 - loss: 0.3851 - val_accuracy: 0.8594 - val_loss: 0.3481\n",
      "Epoch 68/150\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8345 - loss: 0.4034 - val_accuracy: 0.8698 - val_loss: 0.3357\n",
      "Epoch 69/150\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7961 - loss: 0.4103 - val_accuracy: 0.8698 - val_loss: 0.3373\n",
      "Epoch 70/150\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8324 - loss: 0.3764 - val_accuracy: 0.8542 - val_loss: 0.3339\n",
      "Epoch 71/150\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8502 - loss: 0.3523 - val_accuracy: 0.9010 - val_loss: 0.3254\n",
      "Epoch 72/150\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8315 - loss: 0.3765 - val_accuracy: 0.8698 - val_loss: 0.3265\n",
      "Epoch 73/150\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8382 - loss: 0.3641 - val_accuracy: 0.8802 - val_loss: 0.3221\n",
      "Epoch 74/150\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8330 - loss: 0.3612 - val_accuracy: 0.8438 - val_loss: 0.3252\n",
      "Epoch 75/150\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8341 - loss: 0.3532 - val_accuracy: 0.8542 - val_loss: 0.3226\n",
      "Epoch 76/150\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8599 - loss: 0.3475 - val_accuracy: 0.8802 - val_loss: 0.3139\n",
      "Epoch 77/150\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8659 - loss: 0.3431 - val_accuracy: 0.8958 - val_loss: 0.3080\n",
      "Epoch 78/150\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8338 - loss: 0.3966 - val_accuracy: 0.8958 - val_loss: 0.2959\n",
      "Epoch 79/150\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8417 - loss: 0.3633 - val_accuracy: 0.8802 - val_loss: 0.2975\n",
      "Epoch 80/150\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8599 - loss: 0.3132 - val_accuracy: 0.8906 - val_loss: 0.2922\n",
      "Epoch 81/150\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8472 - loss: 0.3407 - val_accuracy: 0.8802 - val_loss: 0.2923\n",
      "Epoch 82/150\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8732 - loss: 0.2944 - val_accuracy: 0.8854 - val_loss: 0.2886\n",
      "Epoch 83/150\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8634 - loss: 0.3039 - val_accuracy: 0.8958 - val_loss: 0.2788\n",
      "Epoch 84/150\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8731 - loss: 0.3284 - val_accuracy: 0.8958 - val_loss: 0.2773\n",
      "Epoch 85/150\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8900 - loss: 0.3009 - val_accuracy: 0.8958 - val_loss: 0.2716\n",
      "Epoch 86/150\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8788 - loss: 0.2995 - val_accuracy: 0.8906 - val_loss: 0.2722\n",
      "Epoch 87/150\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8487 - loss: 0.3367 - val_accuracy: 0.8958 - val_loss: 0.2711\n",
      "Epoch 88/150\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8961 - loss: 0.2992 - val_accuracy: 0.8854 - val_loss: 0.2656\n",
      "Epoch 89/150\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8553 - loss: 0.3322 - val_accuracy: 0.9010 - val_loss: 0.2583\n",
      "Epoch 90/150\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 7ms/step - accuracy: 0.8963 - loss: 0.2989 - val_accuracy: 0.8958 - val_loss: 0.2561\n",
      "Epoch 91/150\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8756 - loss: 0.2984 - val_accuracy: 0.9010 - val_loss: 0.2509\n",
      "Epoch 92/150\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9001 - loss: 0.2811 - val_accuracy: 0.9010 - val_loss: 0.2467\n",
      "Epoch 93/150\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9058 - loss: 0.2626 - val_accuracy: 0.8958 - val_loss: 0.2498\n",
      "Epoch 94/150\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9030 - loss: 0.2544 - val_accuracy: 0.9010 - val_loss: 0.2450\n",
      "Epoch 95/150\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8948 - loss: 0.2630 - val_accuracy: 0.9167 - val_loss: 0.2401\n",
      "Epoch 96/150\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9065 - loss: 0.2387 - val_accuracy: 0.9115 - val_loss: 0.2397\n",
      "Epoch 97/150\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9063 - loss: 0.2397 - val_accuracy: 0.9010 - val_loss: 0.2369\n",
      "Epoch 98/150\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8825 - loss: 0.2660 - val_accuracy: 0.9010 - val_loss: 0.2369\n",
      "Epoch 99/150\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8929 - loss: 0.2514 - val_accuracy: 0.9062 - val_loss: 0.2361\n",
      "Epoch 100/150\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9189 - loss: 0.2178 - val_accuracy: 0.9062 - val_loss: 0.2282\n",
      "Epoch 101/150\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9293 - loss: 0.2150 - val_accuracy: 0.9062 - val_loss: 0.2419\n",
      "Epoch 102/150\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9044 - loss: 0.2270 - val_accuracy: 0.9167 - val_loss: 0.2229\n",
      "Epoch 103/150\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9076 - loss: 0.2553 - val_accuracy: 0.9115 - val_loss: 0.2253\n",
      "Epoch 104/150\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9072 - loss: 0.2431 - val_accuracy: 0.9115 - val_loss: 0.2221\n",
      "Epoch 105/150\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9171 - loss: 0.2375 - val_accuracy: 0.9167 - val_loss: 0.2188\n",
      "Epoch 106/150\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.8995 - loss: 0.2490 - val_accuracy: 0.9219 - val_loss: 0.2112\n",
      "Epoch 107/150\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8950 - loss: 0.2512 - val_accuracy: 0.9323 - val_loss: 0.2136\n",
      "Epoch 108/150\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9103 - loss: 0.2692 - val_accuracy: 0.9167 - val_loss: 0.2113\n",
      "Epoch 109/150\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9102 - loss: 0.2220 - val_accuracy: 0.9062 - val_loss: 0.2232\n",
      "Epoch 110/150\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9059 - loss: 0.2342 - val_accuracy: 0.9167 - val_loss: 0.2165\n",
      "Epoch 111/150\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9084 - loss: 0.2373 - val_accuracy: 0.9271 - val_loss: 0.2045\n",
      "Epoch 112/150\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9278 - loss: 0.2099 - val_accuracy: 0.9375 - val_loss: 0.2013\n",
      "Epoch 113/150\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9207 - loss: 0.1896 - val_accuracy: 0.9115 - val_loss: 0.2107\n",
      "Epoch 114/150\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9019 - loss: 0.2399 - val_accuracy: 0.9323 - val_loss: 0.1906\n",
      "Epoch 115/150\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9392 - loss: 0.2023 - val_accuracy: 0.9271 - val_loss: 0.1974\n",
      "Epoch 116/150\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9064 - loss: 0.2297 - val_accuracy: 0.9479 - val_loss: 0.1954\n",
      "Epoch 117/150\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9273 - loss: 0.1937 - val_accuracy: 0.9323 - val_loss: 0.1897\n",
      "Epoch 118/150\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9219 - loss: 0.2012 - val_accuracy: 0.9427 - val_loss: 0.1837\n",
      "Epoch 119/150\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9246 - loss: 0.1937 - val_accuracy: 0.9479 - val_loss: 0.1799\n",
      "Epoch 120/150\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9312 - loss: 0.1943 - val_accuracy: 0.9271 - val_loss: 0.1900\n",
      "Epoch 121/150\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9281 - loss: 0.2152 - val_accuracy: 0.9271 - val_loss: 0.1838\n",
      "Epoch 122/150\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9453 - loss: 0.1663 - val_accuracy: 0.9271 - val_loss: 0.1820\n",
      "Epoch 123/150\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9494 - loss: 0.1552 - val_accuracy: 0.9375 - val_loss: 0.1829\n",
      "Epoch 124/150\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9454 - loss: 0.1656 - val_accuracy: 0.9479 - val_loss: 0.1760\n",
      "Epoch 125/150\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9489 - loss: 0.1669 - val_accuracy: 0.9427 - val_loss: 0.1750\n",
      "Epoch 126/150\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9392 - loss: 0.1816 - val_accuracy: 0.9427 - val_loss: 0.1698\n",
      "Epoch 127/150\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9286 - loss: 0.1938 - val_accuracy: 0.9375 - val_loss: 0.1743\n",
      "Epoch 128/150\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9312 - loss: 0.1865 - val_accuracy: 0.9271 - val_loss: 0.1798\n",
      "Epoch 129/150\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9383 - loss: 0.1629 - val_accuracy: 0.9427 - val_loss: 0.1661\n",
      "Epoch 130/150\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9437 - loss: 0.1797 - val_accuracy: 0.9479 - val_loss: 0.1684\n",
      "Epoch 131/150\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9443 - loss: 0.1414 - val_accuracy: 0.9427 - val_loss: 0.1693\n",
      "Epoch 132/150\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9460 - loss: 0.1629 - val_accuracy: 0.9427 - val_loss: 0.1625\n",
      "Epoch 133/150\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9385 - loss: 0.1448 - val_accuracy: 0.9427 - val_loss: 0.1620\n",
      "Epoch 134/150\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9538 - loss: 0.1373 - val_accuracy: 0.9479 - val_loss: 0.1598\n",
      "Epoch 135/150\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9690 - loss: 0.1282 - val_accuracy: 0.9427 - val_loss: 0.1669\n",
      "Epoch 136/150\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9481 - loss: 0.1538 - val_accuracy: 0.9427 - val_loss: 0.1686\n",
      "Epoch 137/150\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9483 - loss: 0.1435 - val_accuracy: 0.9479 - val_loss: 0.1592\n",
      "Epoch 138/150\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9420 - loss: 0.1543 - val_accuracy: 0.9427 - val_loss: 0.1650\n",
      "Epoch 139/150\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9519 - loss: 0.1478 - val_accuracy: 0.9427 - val_loss: 0.1600\n",
      "Epoch 140/150\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9527 - loss: 0.1372 - val_accuracy: 0.9479 - val_loss: 0.1568\n",
      "Epoch 141/150\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9598 - loss: 0.1340 - val_accuracy: 0.9427 - val_loss: 0.1539\n",
      "Epoch 142/150\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9583 - loss: 0.1354 - val_accuracy: 0.9531 - val_loss: 0.1549\n",
      "Epoch 143/150\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9637 - loss: 0.1309 - val_accuracy: 0.9479 - val_loss: 0.1520\n",
      "Epoch 144/150\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9450 - loss: 0.1372 - val_accuracy: 0.9427 - val_loss: 0.1646\n",
      "Epoch 145/150\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9578 - loss: 0.1128 - val_accuracy: 0.9479 - val_loss: 0.1518\n",
      "Epoch 146/150\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9579 - loss: 0.1288 - val_accuracy: 0.9375 - val_loss: 0.1623\n",
      "Epoch 147/150\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9473 - loss: 0.1316 - val_accuracy: 0.9531 - val_loss: 0.1541\n",
      "Epoch 148/150\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9553 - loss: 0.1265 - val_accuracy: 0.9479 - val_loss: 0.1485\n",
      "Epoch 149/150\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9602 - loss: 0.1286 - val_accuracy: 0.9479 - val_loss: 0.1560\n",
      "Epoch 150/150\n",
      "\u001b[1m77/77\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9632 - loss: 0.1239 - val_accuracy: 0.9375 - val_loss: 0.1600\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "model4 = Sequential()\n",
    "\n",
    "model4.add(Dense(128, activation='relu', input_shape=(X_train.shape[1],)))\n",
    "## dropout to improve model\n",
    "model4.add(Dropout(0.2))  ## avoid overfitting\n",
    "model4.add(Dense(64, activation='relu'))\n",
    "\n",
    "model4.add(Dropout(0.2))  ## avoid overfitting\n",
    "model4.add(Dense(32, activation='relu'))\n",
    "\n",
    "model4.add(Dropout(0.2))\n",
    "model4.add(Dense(2, activation='softmax'))\n",
    "\n",
    "model4.summary()\n",
    "\n",
    "\n",
    "my_opt = tensorflow.keras.optimizers.Adam(learning_rate=0.001, epsilon=0.1, decay=0.0)\n",
    "\n",
    "model4.compile(loss='sparse_categorical_crossentropy', optimizer= my_opt, metrics= ['accuracy'] )\n",
    "\n",
    "# Define early stopping callback\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5,\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "\n",
    "batch_size= 10\n",
    "epochs = 150\n",
    "\n",
    "history = model4.fit(X_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(X_test, y_test),\n",
    "                    callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjcAAAHHCAYAAABDUnkqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy80BEi2AAAACXBIWXMAAA9hAAAPYQGoP6dpAACF0UlEQVR4nOzdd3gU5drA4d/spvfeICQQeu9dQQ0gIEhTQJqI+Ild1KMcFbuIoqKCoBwpVhABUZQaiiCd0HtNQkghpNdNduf7Y5KFQIAkbPpzX9dezM6885ZwDnl8q6KqqooQQgghRDWhq+gKCCGEEEJYkgQ3QgghhKhWJLgRQgghRLUiwY0QQgghqhUJboQQQghRrUhwI4QQQohqRYIbIYQQQlQrEtwIIYQQolqR4EYIIYQQ1YoEN0KISk9RFN5+++0Sv3fhwgUURWHhwoW3TLd582YURWHz5s2lqp8QonKR4EYIUSwLFy5EURQURWHbtm03PFdVlcDAQBRF4YEHHqiAGgohhEaCGyFEidjZ2fHzzz/fcH/Lli1cvHgRW1vbCqiVEEJcJcGNEKJE+vXrx9KlS8nLyyt0/+eff6Zdu3b4+flVUM2EEEIjwY0QokRGjhzJlStXWL9+vfmewWDgt99+45FHHinynYyMDF566SUCAwOxtbWlUaNGzJgxA1VVC6XLycnhxRdfxNvbG2dnZwYOHMjFixeLzDM6OprHHnsMX19fbG1tadasGfPnz7dcQ4GlS5fSrl077O3t8fLyYvTo0URHRxdKExsby/jx46lduza2trb4+/vz4IMPcuHCBXOavXv30qdPH7y8vLC3t6du3bo89thjFq2rEOIqq4qugBCiagkODqZLly788ssv9O3bF4DVq1eTkpLCiBEj+PLLLwulV1WVgQMHsmnTJiZMmEDr1q1Zu3Ytr7zyCtHR0Xz++efmtI8//jg//vgjjzzyCF27dmXjxo3079//hjrExcXRuXNnFEXhmWeewdvbm9WrVzNhwgRSU1N54YUX7ridCxcuZPz48XTo0IFp06YRFxfHF198wb///sv+/ftxc3MDYOjQoRw9epRnn32W4OBg4uPjWb9+PZGRkebvvXv3xtvbm9deew03NzcuXLjA8uXL77iOQoibUIUQohgWLFigAuqePXvUWbNmqc7OzmpmZqaqqqr60EMPqffcc4+qqqoaFBSk9u/f3/ze77//rgLq+++/Xyi/YcOGqYqiqGfOnFFVVVUPHDigAupTTz1VKN0jjzyiAupbb71lvjdhwgTV399fTUhIKJR2xIgRqqurq7le58+fVwF1wYIFt2zbpk2bVEDdtGmTqqqqajAYVB8fH7V58+ZqVlaWOd2qVatUQJ06daqqqqqalJSkAuonn3xy07xXrFhh/rkJIcqHDEsJIUrs4YcfJisri1WrVpGWlsaqVatuOiT1999/o9free655wrdf+mll1BVldWrV5vTATeku74XRlVVli1bxoABA1BVlYSEBPOnT58+pKSkEB4efkft27t3L/Hx8Tz11FPY2dmZ7/fv35/GjRvz119/AWBvb4+NjQ2bN28mKSmpyLwKenhWrVpFbm7uHdVLCFE8EtwIIUrM29ub0NBQfv75Z5YvX47RaGTYsGFFpo2IiCAgIABnZ+dC95s0aWJ+XvCnTqcjJCSkULpGjRoV+n758mWSk5P59ttv8fb2LvQZP348APHx8XfUvoI6XV82QOPGjc3PbW1tmT59OqtXr8bX15e7776bjz/+mNjYWHP6Hj16MHToUN555x28vLx48MEHWbBgATk5OXdURyHEzcmcGyFEqTzyyCNMnDiR2NhY+vbta+6hKGsmkwmA0aNHM27cuCLTtGzZslzqAlrP0oABA/j9999Zu3Ytb775JtOmTWPjxo20adMGRVH47bff2LlzJ3/++Sdr167lscce49NPP2Xnzp04OTmVW12FqCmk50YIUSqDBw9Gp9Oxc+fOmw5JAQQFBXHp0iXS0tIK3T9x4oT5ecGfJpOJs2fPFkp38uTJQt8LVlIZjUZCQ0OL/Pj4+NxR2wrqdH3ZBfcKnhcICQnhpZdeYt26dRw5cgSDwcCnn35aKE3nzp354IMP2Lt3Lz/99BNHjx5l8eLFd1RPIUTRJLgRQpSKk5MTc+bM4e2332bAgAE3TdevXz+MRiOzZs0qdP/zzz9HURTziquCP69fbTVz5sxC3/V6PUOHDmXZsmUcOXLkhvIuX75cmuYU0r59e3x8fJg7d26h4aPVq1dz/Phx8wquzMxMsrOzC70bEhKCs7Oz+b2kpKQblry3bt0aQIamhCgjMiwlhCi1mw0LXWvAgAHcc889vP7661y4cIFWrVqxbt06Vq5cyQsvvGCeY9O6dWtGjhzJ119/TUpKCl27diUsLIwzZ87ckOdHH33Epk2b6NSpExMnTqRp06YkJiYSHh7Ohg0bSExMvKN2WVtbM336dMaPH0+PHj0YOXKkeSl4cHAwL774IgCnTp3ivvvu4+GHH6Zp06ZYWVmxYsUK4uLiGDFiBACLFi3i66+/ZvDgwYSEhJCWlsa8efNwcXGhX79+d1RPIUTRJLgRQpQpnU7HH3/8wdSpU1myZAkLFiwgODiYTz75hJdeeqlQ2vnz5+Pt7c1PP/3E77//zr333stff/1FYGBgoXS+vr7s3r2bd999l+XLl/P111/j6elJs2bNmD59ukXq/eijj+Lg4MBHH33Eq6++iqOjI4MHD2b69Onm+UWBgYGMHDmSsLAwfvjhB6ysrGjcuDG//vorQ4cOBbQJxbt372bx4sXExcXh6upKx44d+emnn6hbt65F6iqEKExRr+8vFUIIIYSowmTOjRBCCCGqFQluhBBCCFGtSHAjhBBCiGpFghshhBBCVCsS3AghhBCiWpHgRgghhBDVSo3b58ZkMnHp0iWcnZ1RFKWiqyOEEEKIYlBVlbS0NAICAtDpbt03U+OCm0uXLt2wIZgQQgghqoaoqChq1659yzQ1LrhxdnYGtB+Oi4tLBddGCCGEEMWRmppKYGCg+ff4rdS44KZgKMrFxUWCGyGEEKKKKc6UEplQLIQQQohqRYIbIYQQQlQrEtwIIYQQolqpcXNuistoNJKbm1vR1RAWYG1tjV6vr+hqCCGEKCcS3FxHVVViY2NJTk6u6KoIC3Jzc8PPz0/2NhJCiBpAgpvrFAQ2Pj4+ODg4yC/DKk5VVTIzM4mPjwfA39+/gmskhBCirElwcw2j0WgObDw9PSu6OsJC7O3tAYiPj8fHx0eGqIQQopqTCcXXKJhj4+DgUME1EZZW8Hcq86iEEKL6k+CmCDIUVf3I36kQQtQcEtwIIYQQolqR4EbcVHBwMDNnzqzoagghhBAlIsFNNaAoyi0/b7/9dqny3bNnD0888YRlKyuEEEKUMVktZUEZOXnYWOmw1pdvzBgTE2O+XrJkCVOnTuXkyZPme05OTuZrVVUxGo1YWd3+r97b29uyFRVCCCHKgfTcWEhadi7nEzK4kJCB0aSWa9l+fn7mj6urK4qimL+fOHECZ2dnVq9eTbt27bC1tWXbtm2cPXuWBx98EF9fX5ycnOjQoQMbNmwolO/1w1KKovC///2PwYMH4+DgQIMGDfjjjz/Kta1CCCHE7UhwcxuqqpJpyLvtJ89owpBnIinTwInYVNJzcov13q0+qmq5IOm1117jo48+4vjx47Rs2ZL09HT69etHWFgY+/fv5/7772fAgAFERkbeMp933nmHhx9+mEOHDtGvXz9GjRpFYmKixeophBBC3CkZlrqNrFwjTaeurZCyj73bBwcby/wVvfvuu/Tq1cv83cPDg1atWpm/v/fee6xYsYI//viDZ5555qb5PProo4wcORKADz/8kC+//JLdu3dz//33W6SeQgghxJ2Snpsaon379oW+p6en8/LLL9OkSRPc3NxwcnLi+PHjt+25admypfna0dERFxcX89EGQgghRGUgPTe3YW+t59i7fUr0TnJmLheTMlEUhcZ+zuh1pdtAzt7acscEODo6Fvr+8ssvs379embMmEH9+vWxt7dn2LBhGAyGW+ZjbW1d6LuiKJhMJovVUwghhLhTEtzchqIoxR8aUlVQTdi72pGanYshz4RJVXG2sb79u+Xs33//5dFHH2Xw4MGA1pNz4cKFiq2UEEIIYQEyLGUpOWlw+QQkR6AoCi52WkCTmpVXwRUrWoMGDVi+fDkHDhzg4MGDPPLII9IDI4QQolqQ4MZSdFaQlw3ZqWDMxcU+P7jJzrXoqidL+eyzz3B3d6dr164MGDCAPn360LZt24qulhBCCHHHFLUy/uYtQ6mpqbi6upKSkoKLi0uhZ9nZ2Zw/f566detiZ2dX8swvn4TcTHAJQHX04VhMKkaTSj1vJ5xsZQSwIt3x360QQogKdavf39eTnhtLcvDU/sy8ggLXDE3lVlydhBBCiBpGghtLsncHRQd5OZCbiYu91ltTWYemhBBCiOpIghtL0unBzk27zryCk601iqJgyDORkyeTdYUQQojyIMGNpRUMTWUlocdknmuTnGkgy5BHSlYuObnGCqygEEIIUb3JLFdLs3EEvS0YcyA7GRc7J9Kyc4lPyyE+LQcABQVvZ1t8XGzRKaXb4E8IIYQQRZOeG0tTFHDw0K4zEnC1t8ZKp/2YrXQ67Kz0qKjEp2VzJj6dLIP04gghhBCWJD03ZcHBE9JiITcTK0Mqjf1dUVXMxzAkZxq4lJxNdq6Rc5fTaeDrhI1V8Y9aUFWV2NRsTCaVADd7FOn9EUIIIcyk56Ys6K3ByUe7Tr2EDrXQ+VJuDjY08HXCwcYKo6oSmZhVotVUyVm5XE7L4UqGgWyZvyOEEEIUIsFNWXHy1XYtNuZARsINj631Oup42KNXFDINeeb5OLeTazRxKTnL/D1DhrWEEEKIQiS4KSs6PTj7a9dpsWC68YwpGys9tdztAYhPzSYj5/bnUF1KzsJoutrLU5x3iqNnz5688MIL5u/BwcHMnDnzlu8oisLvv/9+x2VbKh8hhBACJLgpWw6eYGUHqlELcIrg5mCDu4MNKhBxJZOUW+xmnJKVS0pWLgoK/q5aUJRpMDJgwADuv//+It/ZunUriqJw6NChElV9z549PPHEEyV653befvttWrdufcP9mJgY+vbta9GyhBBC1FwS3JQlRQGXWtp1xuWbBjgBbvbYW+vJM5mIuJJB5JVMco1XN/3LNZqISckiKjETAG9nGzwdbVAUhVyjibGPjmf9+vVcvHjxhrwXLFhA+/btadmyZYmq7u3tjYODQ4neKS0/Pz9sbW3LpSwhhBDVnwQ3Zc3OBZz8tOu0GEi9BNdNHtbrFEK8nfBxtkVBITnLwPGYVI5dSuVMfDonY9O4nJaDSVVxtLHCx9kOnU7B3lpbYdUz9H68vb1ZuHBhoXzT09NZunQpgwYNYuTIkdSqVQsHBwdatGjBL7/8cstqXz8sdfr0ae6++27s7Oxo2rQp69evv+GdV199lYYNG+Lg4EC9evV48803yc3VeqIWLlzIO++8w8GDB1EUBUVRzPW9fljq8OHD3Hvvvdjb2+Pp6ckTTzxBenq6+fmjjz7KoEGDmDFjBv7+/nh6evL000+byxJCCFGzyVLw21FV7aTvO2HnAnnZWnCTdAEM6eAcoPXs5NMBfvbgoodLyTlk5RrJs7Inz6T14DjYWOHjbIuznZV56bejrZ5MQx45Jhg7diwLFy7k9ddfNz9funQpRqOR0aNHs3TpUl599VVcXFz466+/GDNmDCEhIXTs2PG21TeZTAwZMgRfX1927dpFSkpKofk5BZydnVm4cCEBAQEcPnyYiRMn4uzszH/+8x+GDx/OkSNHWLNmDRs2bADA1dX1hjwyMjLo06cPXbp0Yc+ePcTHx/P444/zzDPPFAreNm3ahL+/P5s2beLMmTMMHz6c1q1bM3HixGL+pQghhKiuJLi5ndxM+DCg3IpzAOrnX2e9HIlBZ4+VXoeDjf6G/WwcbKyAHDJyjDz22GN88sknbNmyhZ49ewLakNTQoUMJCgri5ZdfNr/37LPPsnbtWn799ddiBTcbNmzgxIkTrF27loAA7Wfx4Ycf3jBP5o033jBfBwcH8/LLL7N48WL+85//YG9vj5OTE1ZWVvj5+d20rJ9//pns7Gy+//57HB0dAZg1axYDBgxg+vTp+Pr6AuDu7s6sWbPQ6/U0btyY/v37ExYWJsGNEEIIGZaqzOxtrHB1sMHR1qrIjfocbbRhqZw8I/UbNKRr167Mnz8fgDNnzrB161YmTJiA0Wjkvffeo0WLFnh4eODk5MTatWuJjIwsVj2OHz9OYGCgObAB6NKlyw3plixZQrdu3fDz88PJyYk33nij2GVcW1arVq3MgQ1At27dMJlMnDx50nyvWbNm6PVXNz709/cnPj6+RGUJIYSonqTn5nasHeC/lyyXn6pCahRkJoGiB88QsLa/edm3YKXXYWulJyfPSKbByIQJE3j22WeZPXs2CxYsICQkhB49ejB9+nS++OILZs6cSYsWLXB0dOSFF17AYDBYrFk7duxg1KhRvPPOO/Tp0wdXV1cWL17Mp59+arEyrmVtbV3ou6IomExy8roQQggJbm5PUbTDMC3JqxFcOavNvUmPA48QsLYrVVaOtlpwk2HI4+GHH+b555/n559/5vvvv2fSpEkoisK///7Lgw8+yOjRowHIzTNy8tQpmjVtWqwymjRpQlRUFDExMfj7a3v37Ny5s1Ca7du3ExQUxOuvv26+FxERUSiNjY0NRuOtNx1s0qQJCxcuJCMjw9x78++//6LT6WjUqFGx6iuEEKJmk2GpiqDowL1u/unhBrhyGgwZpcpKm3cDGTlGnJycGD58OFOmTCEmJoZHH30UgAYNGrB+/Xq2b9/O0aPHGPXoBGJj4wptBngroaGhNGzYkHHjxnHw4EG2bt1aKIgpKCMyMpLFixdz9uxZvvzyS1asWFEoTXBwMOfPn+fAgQMkJCSQk3PjrsyjRo3Czs6OcePGceTIETZt2sSzzz7LmDFjzPNthBBCiFuR4Kai6K3Aq4E29GTKgytnIDu1xNk42mrzTrIM2iGcAx8aRVJSEn369DHPkXnjjTdo27Ytffr0oec99+Dm6c09ffoV2kvnVnQ6HStWrCArK4uOHTvy+OOP88EHHxRKM3DgQF588UWeeeYZWrduzfbt23nzzTcLpRk6dCj3338/99xzD97e3kUuR3dwcGDt2rUkJibSoUMHhg0bxn333cesWbNK/LMRQghRMylqSU5srAZSU1NxdXUlJSUFFxeXQs+ys7M5f/48devWxc6udMNEJWYyQuJ5MKQBCrjVAQePYr+uqiqn49NvOECzgY8T9jaFRx0zcvI4dzmdgr9wWys9jfyc77ABVUOF/N0KIYSwmFv9/r6e9NxUNJ0ePOuBvTugQnIEpBd/1Y+iKNT3diLE24na7g7mYark645xMJlULiZloQKu9tYoikJOnlFOFRdCCFHtSHBTGSg6cAsCR2/te2o0pETfsJPxzeh0Co62Vng42uDlZKNlkZXLtZ1ycWnZ5OQZsdLpqOVmj7OtFgTd6iwrIYQQoiqS4KayKDiHyjl/L5mMeG2isbFky7Wd7azRKQo5eSZzr0yu0URCupZPLXdtU0AXe20ptQQ3QgghqhsJbioTRQFnX3AP1npzDBlw+WSJJhrrdQrOdoV7ZRLSclDzz6VyzQ9qXOysUFDIzjWSk2tEVVWik7M4FZuGIU/2ixFCCFF1SXBThAqfY23vDt6NwMpeW0mVeBayU4r9uqu5VyaPPKOJKxlar423y9WTt630OvNKq5SsXKKSsriSnkN2npGE9BuXaFd1Ff53KoQQotxIcHONgl1vMzPv8KBMS7CyA6+GYOeufU+8ADnpt3ylgLPd1QnD0clZmFQVe2u9eZ5NgYIgKC41h+TMq8NfSRmGG/bAMRVzT5zKquDv9PqdjYUQQlQ/Fb5D8ezZs/nkk0+IjY2lVatWfPXVV7c8zDE5OZnXX3+d5cuXk5iYSFBQEDNnzqRfv353XBe9Xo+bm5v5jCIHB4ciz3QqV/Y+kGOA3HSIOwvuQcXazdhBZyQ9J4/kNC1ocXW0u2HTPBvFBHkGTICCgr+bLQlpuRjyjMQnpeHuaIOqqsSkZJOek4e/qx3OdlUrOFBVlczMTOLj43Fzcyt0HpUQQojqqUKDmyVLljB58mTmzp1Lp06dmDlzJn369OHkyZP4+PjckN5gMNCrVy98fHz47bffqFWrFhEREbi5uVmsTgUnVleqQxhVE6SngjEHouNAbw16G+1MqpucS5VpyCMxQ5tzY61XsM6040oRcVpqhoGcXCPujjYkZOm1gCgzlyS9go+zHRkG7TvA5Uvg5WSLjVXV6/Bzc3O75WnkQgghqo8K3cSvU6dOdOjQwbz7rMlkIjAwkGeffZbXXnvthvRz587lk08+4cSJE6UeXijuJkBGo5Hc3Eq0kigrGf58HuKPFr7/wJcQ3PWG5Bk5uQyds4Nco4lX+jTi/ub+RWZrMqlk5xnN++NkGvIY8e1OMnLyeKx7XX7YHkGuyYSPsx3xadl4Otnw9ah2eDnZFplfZWRtbS09NkIIUcWVZBO/CgtuDAYDDg4O/PbbbwwaNMh8f9y4cSQnJ7Ny5cob3unXrx8eHh44ODiwcuVKvL29eeSRR3j11Vdv+ssrJyen0HBMamoqgYGBxfrhVDqqCilRcOkAHFoCJ1ZpS8ef2p6/CWBhy/Zd5HR8Oi/1boi1vvi9Le+vOsb/tp03fw9t4sOnD7dm2JztnI5Pp0UtV379vy7Y20jAIIQQonxUiR2KExISMBqNNxyG6OvrS2xsbJHvnDt3jt9++w2j0cjff//Nm2++yaeffsr7779/03KmTZuGq6ur+RMYGGjRdpQrJf94hqYDYcg87TTxtEuw+sZeLoCh7WrzWt/GJQpsAMZ1DUaXP4Tl62LLx8Na4WpvzXfjOuDuYM3h6BRmbTp9p60RQgghykSVmjxhMpnw8fHh22+/pV27dgwfPpzXX3+duXPn3vSdKVOmkJKSYv5ERUWVY43LkI0DDJ6r7YdzaDEc/9NiWQd6ODCsXW3srfV8MaINHo7arsd1PB34aGhLAOZtPU/klUqwqkwIIYS4ToUFN15eXuj1euLi4grdj4uLu+nET39/fxo2bFhoCKpJkybExsZiMBS9k6+trS0uLi6FPtVGYEfo9rx2/ecLcOWsxbKePrQl+6f2onM9z0L3ezf1pXt9Lwx5Jt7/61iR76Zl57L+WJxsBiiEEKJCVFhwY2NjQ7t27QgLCzPfM5lMhIWF0aVLlyLf6datG2fOnMFkuvpL89SpU/j7+2NjY1Pmda6Uek4BvxaQmQALH4DEcxbJVlEU7KxvnFOjKApvDWiKXqew7lgc204nFHqeazQx+rvdTPx+Ly/+eqDK748jhBCi6qnQYanJkyczb948Fi1axPHjx5k0aRIZGRmMHz8egLFjxzJlyhRz+kmTJpGYmMjzzz/PqVOn+Ouvv/jwww95+umnK6oJFc/KFkavAO/G2vybhQMg6UKZFtnA15kxnYMAeHfVUXKNV4PNz9af4mBUMgB/HYphZpjMzRFCCFG+KjS4GT58ODNmzGDq1Km0bt2aAwcOsGbNGvMk48jISGJiYszpAwMDWbt2LXv27KFly5Y899xzPP/880UuG69RnLxh7B/g2QBSL8J3fWDH7BId2VBSL4Y2xN3BmlNx6Yz+3y6iEjPZdjqBuVu0obGhbWsD8GXYaVYeiC6zegghhBDXq9B9bipCSZaSVTmpMbBogHaaOICNM7QdC3dNBkcvixe3/lgczy/eT6bBiKONHltrPYkZBkZ2rMO0IS2Ytvo432w5h42VjmVPdqVFbVeL10EIIUTNUCWWgosy4OIPT26FB2aCVyMwpMHO2fBFa/hnBhgsu7qpV1NfVj9/Fx2C3ckwGEnMMFDfx4mpDzQF4NU+jQlt4oshz8QHfxc9+VgIIYSwNOm5qa5UFc6EwcZ3Ieagds/OFWq1A/9WEHIf1L3LIkUZTSrzt53nn9OXmfpAUxr4OpufXUrOoucnmzEYTfz8eCe61rd8D5IQQojqr0rsUFxRakxwU8BkgqPLIewdSI4s/OzBr6HNqDKvwtt/HGXh9gu0rePGskldK/4wUiGEEFWODEuJq3Q6aDEMng2HiZvggc+h8QPas78mQ8yhMq/CU/eEYGetIzwymc0nL5d5eUIIIWo2CW5qCr011GoL7R+Dh3+A+r0gLxt+HQNZSWVatI+zHeO6BAMwY91JalhnoRBCiHImw1I1VWYifNtDG6pqeD+M+Bl0ZXcQZmKGgbumbyTDYMTJ1gprvYKTnRWfDGt1wy7IQgghxPVkWErcnoOH1oOjt4VTa+CXEWW6L46How1P31sfgPScPJIyc4lKzGLWxjNlVqYQQoiaSXpuarrjf8Kyx7UhKq9GMPIX8Awps+JiU7LJyjUSn5rN8G93oijwzyv3EOjhUGZlCiGEqPqk50YUX5MBMH41OPtDwkmYdy8cWqotJS8Dfq521PVypFM9T7rV90RVYVn4xTIpSwghRM0kwY3QJhpP3KTtgZOdDMsf14apUsr22ISH2wcCsHTvRTlgUwghhMVIcCM0Lv4wfg3c8wbobbR5OLM7wd752l45ZaBPMz+c7ayITs5i+9krAPy27yJdpoWx+nDMbd4WQgghiibBjbjKygZ6vAL/txVqd9COb1j1Inw/EK6ctXhxdtZ6HmwdAMCve6P4eVckLy89SExKNh+uPk6esWyCKiGEENWbBDfiRj6N4bG1cP9HYO0AF7bCnG5w9HeLF1UwNPXX4Rj+u+IwAHqdQlRiFn8fibV4eUIIIao/CW5E0XR66DwJJm2HundDXhYsfRR2fWPRYlrUcqWxnzPG/Dk3T9xdj+fvawDAnM1nZcM/IYQQJSbBjbg1j7ow5ndoPwFQYfV/YN2bFpuHoygKT/YIwUqn8Ny99ZnStzFjuwThYKPneEwq/5xOsEg5Qgghag4JbsTt6fTQ/1O4b6r2ffuXsOp5iwU4g9rU4vh79zO5dyMURcHNwYaRHesAMGfzjZv87YtI5PnF+9l17opFyhdCCFG9SHAjikdR4K6XYNAcUHQQ/j2sfBpMRotkb60v/D/FCd3rYqVT2HkukaV7o0jONJCek8dbK48wbO4OVh64xOOL9nL2cnqR+WUZjHy9+QwnYlMtUj8hhBBVh+xQLEru8G+w/AlQjdB8mBbwWNlYvJiXlx7kt31XN/hztNGTYdCCKV8XW+JSc6jv48TvT3fDydaq0LtvrTzCoh0RtAp0Y+XT3SxeNyGEEOVLdigWZavFMHhoAeis4Mhv8G1PuLTf4sW8+UBTRneuQz1vRwAyDEbqeDjw0+Od+PPZ7vi62HImPp2Xfj1QaBPAA1HJfL8zAoCDUcnEpWZbvG5CCCEqL+m5EaV3ej2s+D/IvAKKHro9Dz1fAytbixeVmGHgwpUMmvq7YGetnV4eHpnEiG92YjCaGNM5iDceaIJeURgw61+Ox1wdjvpgcHNGdQqyeJ2EEEKUH+m5EeWjQS94ejc0H6oNUW37DBb0gxTLnxXl4WhD2zru5sAGoG0dd94b1AyAH3ZGMHj2dj74+zjHY1Jxc7BmQve6AKw/Fmfx+gghhKi8JLgRd8bRC4bNh+E/gp0bRO+Fb+6Gc5vLpfjhHeowb2x73B2sORaTyoJ/LwDw375NGNFB2yBw+5krpOfklUt9hBBCVDwJboRlNBkA/7cF/Fpqw1Q/DIaN74Mxt8yL7tXUlzUv3E33+l4AdK7nwUPta1Pfx4kgTwcMRhNbT10GID0nj+lrTrDjrCwjF0KI6kqCG2E57sEwYR20GQ2qCf75BP4XCpdPlnnRvi52fP9YR5ZN6srC8R1RFAVFUejVxBfQhqZUVeXVZYeYs/ms+agHIYQQ1Y8EN8KyrO3hwdkwbIE2TBVzQBum+vdLMJbt0JBOp9AuqPC8nNCmWnCz8WQ83207z1+HtNPGzydkEHkls0zrI4QQomJIcCPKRvMh8NQOCLkX8rJh/Zvwv3sh5lC5VqN9kDtuDtYkZ+by/l/HAcx74mw5fblc6yKEEKJ8SHAjyo5LAIxeDgNngZ0rxBzU9sT58wVIiS6XKljpddzbyMf8vX9Lfyb1DAHgn1MS3AghRHUkwY0oW4oCbcfA03ug6SBtyfi+BfBlG1jzX8hJK/MqPNDKH4B63o5MH9qSuxt4A7Dj7BUMeZY5H0sIIUTlIcGNKB/OvvDwIhi/Gup0AWMO7JwNPz0EhrKd+3JvY19+frwTyyd1xcnWimYBLng62pCek0d4ZFKZli2EEKL8SXAjyldQVy3AeWQp2LpC5A74dQzkGcq02K71vXBz0M6/0ukU7mqgLRuXoSkhhKh+JLgR5U9RoGFvGLUUrB3gzAZY/niZr6a61t0NtaGpf/InFSek5/DaskOsPFA+c4GEEEKUHQluRMWp0wlG/AR6Gzi2Er65C47+DqaynwdzV/68myPRqZyMTWPktztZvCeKV5Ye4mKSLBEXQoiqTIIbUbFC7oWHFmpDVPHHYOk4mNutTE4Zv5a3sy1N/bWD14Z8/S+n49MBMBhNfLHhdKG0l9NyyDIYy7Q+QgghLEeCG1HxGveHFw5Bj9euBjnfD4K4Y2VabI9GWu9NhsGIv6sdM4e3BmBZ+EXOxGuruDYci6PbRxsZO39XmdZFCCGE5UhwIyoHeze4Zwq8cBBqd4DsZPhhECSeK7Mi+zTzAyDA1Y7FT3RmUJta9Grqi0mFGWtPse10Ak/9HI7BaGLPhSSSMsp20rMQQgjLUFRVVSu6EuUpNTUVV1dXUlJScHFxqejqiKJkJcGC/hB/FNzqwPg14FqrTIo6Ep1CoIcDrvbWAJyMTeP+L/5BVcHOWkd27tX5P/8b2958nIMQQojyVZLf39JzIyofe3cYswI86kFyJHzXC2LL5qDL5rVczYENQCM/Zwa31gKp7FwTPRp6M6St9n1PRGKZ1EEIIYRlSXAjKidnXxj7B3g1hNRo+K4PnFxTLkW/2KshXk423NXAi7mj29E1RNsTZ+8F2fBPCCGqAgluROXlFggT1kPdHpCbAYtHwvavoIxHUgM9HNj931C+f6wj9jZ6OgS7A3D4YgrZuTeumsrJMzJ2/m5eW1a+h4IKIYQomgQ3onKzd4PRy6DtWFBNsO4N+O0xMGSUabE6nYKiKADU8XDAy8kWg9HE4eiUG9JuPnmZf05dZvGeKK6k55RpvYQQQtyeBDei8tNbw4Avoe8noLOCo8vhf6GQeL5cilcUxdx7s+fCjfNu/j4cY74+eDG5XOokhBDi5iS4EVWDokCnJ2DcKnDy1fbCWfiANuG4HLQP9gBg33XzbrJzjWw4Fmf+fiAyuVzqI4QQ4uYkuBFVS1AXeGILeDaA1IuwaCCkxZZ5se2DtJ6bvRFJmExX5/xsOXWZjGt2L94flVzmdRFCCHFrEtyIqsfFH8b9AW5BkHQevn8QMhLKtMimAS7YW+tJycrlzOV08/2/DmlDUp3raT07B6OSCwU/Qgghyp8EN6JqcgnQAhznALh8Ahb0K9MhKmu9jjZ13ICrS8Kzc42EHdeGpCb3aoStlY7U7DzOX7k62TklK5e07Nwyq5cQQogbSXAjqi73YBi7UgtwEk5qk4xjDpZZceahqfxJxZtPakNStdzs6RDsTvNarsDVeTep2bn0+mwL/b7ciiGv7E86F0IIoZHgRlRt3g3h8Q3g0xTS47QenLD34OjvcOWsRYsqmFS86lAMU5Yf4qddEQD0be6Hoii0DnQD4ED+vJvl+y4Sn5ZDVGIWu85fsWhdhBBC3JwEN6Lqc60Fj62BuneDIR22zoCl4+CrtrDmvxYrpnM9T+5u6I3BaOKX3VFsPa3N8+nf0h+gUHCjqio/7ro6TLb+mhVVQgghypYEN6J6sHOFUctgwBfQZgz4t9bu75wNkbssUoSNlY5F4zvw6/91oU8zXxQFmgW4mIOagj+Px6Sy5dRlzsRfnXi84VgcNeyMWiGEqDByKriovlY+A/t/AL+W8MRm0Oktmn1ShgFbax0ONlYAqKpKhw82kJBuoK6XI+cTMhjStharD8eSlWtk1bPdzfNyhBBClEyVOxV89uzZBAcHY2dnR6dOndi9e/dN0y5cuBBFUQp97OzsyrG2osq47y2wdYXYQ7BvocWzd3e0MQc2QKF5N+cTtBVTE7rX5e6G2sGbMjQlhBDlo8KDmyVLljB58mTeeustwsPDadWqFX369CE+Pv6m77i4uBATE2P+RERElGONRZXh5A33vq5db3wPMm88OsHSCoIbgLZ13GgW4EpoE18ANhyX4EYIIcpDhQc3n332GRMnTmT8+PE0bdqUuXPn4uDgwPz582/6jqIo+Pn5mT++vr7lWGNRpbSfAD7NICsJfhwCh5ZCbnaZFdc60N18PbpzEAD3NvZBp8DRS6lEJ2eVWdlCCCE0FRrcGAwG9u3bR2hoqPmeTqcjNDSUHTt23PS99PR0goKCCAwM5MEHH+To0aM3TZuTk0Nqamqhj6hB9FbwwGegt4VL+2H54/BpQzjwc5kU17qOG15ONgR62NOvhbaKytPJlnb5e+RskKEpIYQocxUa3CQkJGA0Gm/oefH19SU2tujzgho1asT8+fNZuXIlP/74IyaTia5du3Lx4sUi00+bNg1XV1fzJzAw0OLtEJVcnc7w7D7oOQVcAyE7Bf58AZIuWLwoJ1sr1r3Yg1XP3IWd9dUJzL2aytCUEEKUlwofliqpLl26MHbsWFq3bk2PHj1Yvnw53t7efPPNN0WmnzJlCikpKeZPVFRUOddYVApugdDzNXj+INTtAcYcWPdGmRTl4WiDq4N1oXu9mvoBsOPsFVIy5TgGIYQoSxUa3Hh5eaHX64mLK/xfs3Fxcfj5+RUrD2tra9q0acOZM2eKfG5ra4uLi0uhj6jBdHroOx0UPRz/E85uKpdi63o50tDXiTyTKr03QghRxio0uLGxsaFdu3aEhYWZ75lMJsLCwujSpUux8jAajRw+fBh/f/+yqqaobnyaQMeJ2vWa18BYPj0pfZtr/xtdfSSmXMoTQoiaqsKHpSZPnsy8efNYtGgRx48fZ9KkSWRkZDB+/HgAxo4dy5QpU8zp3333XdatW8e5c+cIDw9n9OjRRERE8Pjjj1dUE0RV1PM1sPfQThT/dyaUw16WBcc0/HMqgVQ5KVwIIcpMhQc3w4cPZ8aMGUydOpXWrVtz4MAB1qxZY55kHBkZSUzM1f/STUpKYuLEiTRp0oR+/fqRmprK9u3badq0aUU1QVRF9u5w31TteuP78OtYSL9cpkU28HEixNsRg9HExuM338epKO+tOsaIb3eQkZNXRrUTQojqQ45fEDWXqsKWj+Gfj8GUBw6e8ODX0Oj+Mivy03Un+WrjGXo39eXbse2L9U5GTh4t31mH0aTy1cg2DGgVUGb1E0KIyqrKHb8gRIVQFOj5KkzcqG30l3kFloyCE3+VWZEFe99sPnWZ9GL2why8mIzRpP03yMYTJevxEUKImkiCGyH8W2kHa7Z4WOvBWfoonN5QJkU19nOmrpcjhjxTsQOV8Igk8/Xmk/HmQEcIIUTRJLgRAsDKBgbNgaYPgtGg9eCUwTJxRVHo21zb5uCHHReY9vdxhs3ZznO/7CfPaCrynfDIZPN1UmYuB6KSikwnhBBCI8GNEAX0VjD0O2jYF/Ky4adhsH2WxVdSFQxN7bmQxDf/nGNvRBJ/HLzE3ogbgxZVVQmP1O7X83IEIKyEk5GFEKKmkeBGiGvpreHhRdB8qDZEte51WDIaspItVkSzABcGt6lFI19nRnYMpGOwB1D0uVPnEjJIzszF1krHpJ4hgMy7EUKI25HgRojrWdlqPTj9ZoDeBk6sgm97QMxBi2SvKAqfD2/N2hfvZtqQlozvFgxo505dv3hxX35vTsvaroQ28UWnwInYNDldXAghbkGCGyGKoijaLsaPrQW3Otohm//rBXsXWHyY6q6G3tjodVy4ksnZyxmFnu3PH5JqG+SOu6ON+XRx6b0RQoibk+BGiFup1Rb+7x9tHo4xB1a9AH+/YtEinGyt6BziCdx4anhBz027OlpQc29jbXPLjXI+lRBC3JQEN0Lcjr07jPgZQt8BRQd75sGZsNu/VwK9mvgAhefdpGTlciouHdB6bgDubayl2372iuxWLIQQNyHBjRDFodNB9xeg05Pa979fgbwci2V/XxOtRyY8Mokr6Vq+B6KSAQjydMDLyRaAhr5OBHk6kJNn4o3fj9wwR0cIIYQEN0KUTM/XwMkXEs/C9q8slm2Amz3NAlwwqbDppHbG1fVDUqBNRp42pAV6ncKK/dHM//eCxeoghBDVhQQ3QpSEnSv0fl+7/mcGJEdaLOvQ/N6bNUdi2B+ZxOaT2qThNkHuhdJ1DfHi9X5NAPjw7+NsP5NgsToIIUR1IMGNECXV4iEI6gZ5WbBooHai+OpX4fiqO1pJVRDcbDgez+Cvt3PoYgoAHYLdb0g7vlswQ9rWwmhSefrncOLTsktdrhBCVDcS3AhRUoqi7YFj7QBJ5+HYStg1Vzuy4ceh2rLxUmhey4Um/tpJt15ONvRo6M3bA5rS2O/G028VReHDwS1o4u9CUmYuX286eyctEkKIakVRa9iMxJIcmS7ELaVEw8XdkBYHV85A+PfacnEre7j3DejytBYIlUBOnpHUrDy8nGxQivHuttMJjP5uFzZ6HZtf6UmAm31pWyOEEJVaSX5/S8+NEKXlWguaDYbOT0L/GTBpOwTfpQ1XrXsdVvwf5JZsuMjWSo+3s22xAhuAbvU96VzPA4PRxFcbz5SmFUIIUe1IcCOEpXjVh3F/akNWih4OLYHvB0L65TIrUlEUXurdCICle6OIuJJxQxqTSSUlM7fM6iCEEJWNBDdCWFLBsQ2jl2krq6J2wfzekJ1SZkV2CPagR0Nv8kwqX2w4fcPzV5cdou376zkVl1ZmdRBCiMpEghshykLIPfB4GLgGQuI5WPWixc+kutbL+b03Kw5EE3kls9Czf88kYDSp7D6fWGblCyFEZSLBjRBlxasBDFugDVEdWQYHfi6zolrUdqVtHTdUFfZHJZnvZxmMXErR5v2cT7hxyEoIIaojCW6EKEuBHeCe/2rXf78CCWe0ScYpF8GQeet3S6iRnzMAZ+PTzfcuXDMH54IEN0KIGsKqoisgRLXX/UU4txkubIWvO4Ep/8BL5wB4di/YOFqkmBBvJwDOXr4axFzbW3O+iMnGQghRHUnPjRBlTaeHId+Co/fVwAYg7RKcWmuxYkJ8tODmzDU9N9cGN1GJmeQZTRYrTwghKisJboQoDy4B8MxeeGYfvBoB3V7Q7h9dYbEi6uf33JxPyMBo0iYvn7umFyfXqHIpWY5pEEJUfxLcCFFe7N20vXDs3aD5UO3e6XWQY5kl2gFu9tha6TAYTVxM0ubznEtIL5RGhqaEEDWBBDdCVAS/FuARAnnZcHKNRbLU6xTqemnzd85e1oKagmGp2u7asQwyqVgIURNIcCNERVAUaD5Eu7bk0FT+vJuz8RkkZRhIzt+Z+J5GPoAsBxdC1AwS3AhRUZrlBzdn1ltsB+OCFVNn4tM5lx/IBLjamU8bvyDDUkKIGkCCGyEqik8T8GoERgOcXG2RLAtWTJ29nG7upanr7UiwlwMgw1JCiJpBghshKsq1Q1MHf4HLpyA5EvIMpc4yxPvqnJvz+ZOJ63o5mufiRCVlkSvLwYUQ1ZwEN0JUpGaDtT/PbYbZHWBmC/iiJcQeLlV29bycUBRIysxlX4R2DENdLyd8ne2ws9ZhNKlcTMqyUOWFEKJykuBGiIrk3QjaP6btVmzvDjprSIuBRQMg5lCJs7O30VPLTVsZteeCFtzU83JEp1MI9tR6b2RoSghR3UlwI0RFe+BzeOk4vHoB/nMWarWHrCT4fiDEHCxxdgWTigs28isYkioIbgrm4phMKvFpsqmfEKL6keBGiMrEzhXGLL8mwBkEmYklyqIguAGw0inmPW6C84OcC1cyUFWVZ34Jp+MHYczaeLrQ+8cupfLdtvPk5BnvrC1CCFFBJLgRorIpCHC8GkFWIoR/X6LXQ3yuHsRZx9MBK732f/O6+SumzidksO5YHH8fjgVgxrpTfLb+FKqq8sOOCwya/S/vrTrGnM1nLdQgIYQoXxLcCFEZ2blC12e1673zwVT8XpT61/Tc1PO6GugUDEudjkvn3T+PAdAq0A2AL8NO0+/Lbby58iiG/NVUS/dexJQ/tCWEEFWJBDdCVFbNh4KdGyRHwJkNxX6tYK8buDrf5trr2NRsopOzqOVmz+KJnXnzgaYAHI9JxUqn8Or9jXG2syI6OYsd565Ypi1CCFGOJLgRorKycYA2o7Xr3fOK/Zqnow2u9taAtgy8gLezLY42evP3twc2w95Gz4TudZnxUCvuauDFkv/rzKSeIQxsFQDAr3ujLNAQIYQoXxLcCFGZtX9M+/PMBkg8V6xXFEWhQ7A7AG3quBW6XzCpOLSJD72a+pqfDWtXmx8mdKJdkAcAwzsEArD6SCwp+edTCSFEVSHBjRCVmWcI1A8FVNjzXbFf+2pkWza93NN8plSBJ+6uR4+G3rz7YPNbvt+iliuN/Zwx5Jn449ClG57HpWYze9MZUrIk8BFCVD4S3AhR2XV4XPtzz/9gdieY1QEW9IO9CyA7tchX7G30hebbFHiwdS0WPdaRgPyN/m5GURQeaq/13iwtYmjqvVXH+GTtST7461gJGyOEEGVPghshKrsGvcGzAeRlw+UTkHAKIv6FVS/Ap41g1WTItfxmfINaB2CtVzh0MYXjMVeDqIycPDYcjwNgxf5oYlLkOAchROUiwY0QlZ1ODxPWwdg/YNwqePQv6P0+eDWE3EzY+x2sedXixXo62RLaRJuX8+POCPP9DcfjyM7VlovnGlX+t/W8xcsWQog7IcGNEFWBgwfU6wF174Lg7toeOE/vhocWAQrsWwjhP1i82LFdggFYHh5tnli86lAMAG3zJyv/sjuSpIwbTzL/fX80bd9bz6oi5uwIIURZkuBGiKpKUaDZILj3de37Xy/Bpf0WLaJzPQ8a+zmTlWtk8Z5IUrJy2XLyMgAfDmlBU38XMg1GFu24UOi9vRcSeeW3gyRmGJi54TSqKpsBCiHKjwQ3QlR13V+Chn3BmANLxkL6ZYtlrSgKj3WvC8Ci7RdYfTgGg9FEQ18nGvu5MKlnCAALt18g05AHwMWkTP7vh33kGrWA5kx8uvmEciGEKA8S3AhR1el0MHgueNSDlEj4+WHISbdY9gNbBeDpaMOllGw+WnMCgAdaapv89WvhT7CnA8mZudz98Wae+2U/jy3cw5UMA80CXBjUWkv3066Im+YvhBCWJsGNENWBvRs88ivYe8ClcFg6DoyW2YPGzlrPqE51AEjOn3fzQEt/APQ6hbcHNsPRRk9Ceg5/HLzEqbh0vJxsmTe2PRO61wNg9eFYrqTnWKQ+QghxOxLcCFFdeDWAUUvByl7b0fiP58CYZ5GsR3cOwlqvANAswIV61xzO2bORD+FTe/HLxM48e299+jTzZeH4DgS42dOitista7tiMJr4bd9Fi9RFCCFup1TBTVRUFBcvXv2Havfu3bzwwgt8++23FquYEKIUareHhxaCooeDP8Pc7nBu8x1n6+Nix+A2tQAY2rb2Dc9trfR0CfHkpd6N+GZMe5rXcjU/K+j1+Xl3pJwyLoQoF6UKbh555BE2bdoEQGxsLL169WL37t28/vrrvPvuuyXOb/bs2QQHB2NnZ0enTp3YvXt3sd5bvHgxiqIwaNCgEpcpRLXV6H4Y9p02RHX5OHz/ICwZc8fzcN59sDk/TOjIo12DS/TegFYBONtaEXElk3/PJhR6Fp+azaQf97EvIvGO6iaEENcqVXBz5MgROnbsCMCvv/5K8+bN2b59Oz/99BMLFy4sUV5Llixh8uTJvPXWW4SHh9OqVSv69OlDfHz8Ld+7cOECL7/8MnfddVdpmiBE9dZsMDwXDp2e1Hpxjv8Bmz64oyztrPXc1cAbnU4p0XsONlYMyu/1+fNg4T1vftgZweojsczccPqO6iaEENcqVXCTm5uLra0tABs2bGDgwIEANG7cmJiYmBLl9dlnnzFx4kTGjx9P06ZNmTt3Lg4ODsyfP/+m7xiNRkaNGsU777xDvXr1StMEIao/e3foOx1GLta+75oLMQcrpCo9G3kDEB6ZXOj+/vzvB6OSZS8cIYTFlCq4adasGXPnzmXr1q2sX7+e+++/H4BLly7h6elZ7HwMBgP79u0jNDT0aoV0OkJDQ9mxY8dN33v33Xfx8fFhwoQJty0jJyeH1NTUQh8hapSGvaHZEFBN8OcLYDKWexVaB7oB2p43BTsdm0wqB6OSAUjNzuPClcxyr5cQonoqVXAzffp0vvnmG3r27MnIkSNp1aoVAH/88Yd5uKo4EhISMBqN+Pr6Frrv6+tLbGxske9s27aN7777jnnz5hWrjGnTpuHq6mr+BAYGFrt+QlQbfT4EG2dtmfi+BeVevKeTLcGeDgDsj9I29Dt7OZ20nKuruQoCHSGEuFOlCm569uxJQkICCQkJhYaPnnjiCebOnWuxyl0vLS2NMWPGMG/ePLy8vIr1zpQpU0hJSTF/oqKiyqx+QlRaLv5w35va9YZ3Ia3o/3goS23quANXh6L2XzdEdUCCGyGEhViV5qWsrCxUVcXdXfvHKiIighUrVtCkSRP69OlT7Hy8vLzQ6/XExcUVuh8XF4efn98N6c+ePcuFCxcYMGCA+Z7JpJ1ObGVlxcmTJwkJCSn0jq2trXl+kBA1WofH4cDPEHMAfp8Eo5ZpuxuXk7Z13FixP5rwSK3nZn9+MFPLzZ7o5CwJboQQFlOqf9kefPBBvv/+ewCSk5Pp1KkTn376KYMGDWLOnDnFzsfGxoZ27doRFhZmvmcymQgLC6NLly43pG/cuDGHDx/mwIED5s/AgQO55557OHDggAw5CXErOj0M/kbb5O/sRtgxq1yLL+i5ORCVjMmksj8/yBnbJQiAY5dSMeSZyrVOQojqqVTBTXh4uHkJ9m+//Yavry8RERF8//33fPnllyXKa/LkycybN49FixZx/PhxJk2aREZGBuPHjwdg7NixTJkyBQA7OzuaN29e6OPm5oazszPNmzfHxsamNM0RoubwaQz3T9Ouw96B6H3lVnRjP2fsrHWkZedxKDqFU3FpADzYuhZuDtYYjCZOxMqEfyHEnStVcJOZmYmzszMA69atY8iQIeh0Ojp37kxERMkOyBs+fDgzZsxg6tSptG7dmgMHDrBmzRrzJOPIyMgSLy8XQtxCu0ehyUAw5cFvEyC7fAIKK72OlrXdAPh++wVMKgS42uHnaker/PsyqVgIYQmlCm7q16/P77//TlRUFGvXrqV3794AxMfH4+LiUuL8nnnmGSIiIsjJyWHXrl106tTJ/Gzz5s233Bhw4cKF/P777yUuU4gaS1Fg4JfgUhuSzsPPw8FQPsuw2+YPTf15SNvMr3UdNwBa5S8VPxCVUi71EEJUb6UKbqZOncrLL79McHAwHTt2NM+PWbduHW3atLFoBYUQZcDeHUb8BLYuELkdloyCvLI/tbttfjCTa9Q27GsTqAU7rQO1s6gO5C8TF0KIO1Gq4GbYsGFERkayd+9e1q5da75/33338fnnn1usckKIMhTQGkb9BtaO2gTjpePBmFumRRZMKi5g7rnJH5Y6ezmD1OyyrYMQovor9TpQPz8/2rRpw6VLl8wnhHfs2JHGjRtbrHJCiDJWpxOM/AX0tnDyL/h1XJn24Hg72xLoYQ+AlU6heYDWY+PpdPX+4YsyNCWEuDOlCm5MJhPvvvsurq6uBAUFERQUhJubG++995553xkhRBVRrweM+PlqgPPLiDKdg1MwFNXY3xl7G735fkHvzYJ/zzPvn3P8sOMC8anZZVYPIUT1Varg5vXXX2fWrFl89NFH7N+/n/379/Phhx/y1Vdf8eabb1q6jkKIstYgFEb9CtYO2hDVT8PAkFEmRfVupq2E7NWk8EadBZONNxyP54O/j/PmyqOMW7BH9r4RQpSYopbiKN6AgADmzp1rPg28wMqVK3nqqaeIjo62WAUtLTU1FVdXV1JSUkq1skuIai1yJ/z0EOSkQtdnoff7ZVLMhYQMarvbY6W/+t9X6Tl5fL3pDFfSDRiMJjaeiCclK5fn72vAi70alkk9hBBVR0l+f5cquLGzs+PQoUM0bFj4H5yTJ0/SunVrsrKySppluZHgRojbOLkGfhkOeht4Zi+4B1VINf48eIlnf9mPlU7h96e70byWa4XUQwhROZTk93ephqVatWrFrFk3bt0+a9YsWrZsWZoshRCVRcM+UPduMBpg43sVVo0HWvrTt7kfeSaVl5celOEpIUSxlarnZsuWLfTv3586deqY97jZsWMHUVFR/P333+ajGSoj6bkRohhiDsI3PQAVJm6CWm0rpBoJ6Tn0/vwfEjMMvBjakOdDG1RIPYQQFa/Me2569OjBqVOnGDx4MMnJySQnJzNkyBCOHj3KDz/8UKpKCyEqEf9W0HK4dr3uTSj5fwNZhJeTLVMfaArADzsvkGeU3hshxO2VqufmZg4ePEjbtm0xGo2WytLipOdGiGJKjoJZ7SEvG0YuhkZ9K6QauUYTnT8M40qGgQWPduCexj4VUg8hRMUq854bIUQN4BYInZ7Urjd+ABW0h5W1XsfA1gEALAu/WCF1EEJULRLcCCFurtvzYOMMcYfh+MoKq8aQNrUBWH8sTo5nEELclgQ3Qoibc/CALk9r15umgSl/yDn8e5jTDS7uLZdqNK/lQgMfJ3LyTKw+HFMuZQohqi6rkiQeMmTILZ8nJyffSV2EEJVRl6dg11xIOAlHlkFSBGzK39xv66fa2VRlTFEUhrStzfQ1J1gWHs3wDnXKvEwhRNVVouDG1fXWm2i5uroyduzYO6qQEKKSsXPVdive+B788RzkXbNJ5+l1kH4ZnLzLvBqD2gTw8doT7D6fSFRiJoEeDmVephCiaipRcLNgwYKyqocQojLr9CTs/Boyr2jf+34MBxfDpXA4vFTr3Slj/q72dA3x5N8zV1ixP5rn7pM9b4QQRZM5N0KI27N1gv6fglcjGPoddPo/aP2I9uzgz+VWjaFttYnF8/45R+SVsju5XAhRtUlwI4QonmaD4Znd0GKY9r35UO38qdjDEHukXKowsFUA7YPcScvJ45lfws1HMhy7lMq4+btZtP1CudRDCFG5SXAjhCgdBw9oeL92fbDsJxUDWOl1fDmyDW4O1hy6mMJHq0/wy+5IBn39L1tOXebtP4+y+3xiudRFCFF5SXAjhCi9gqGpQ0vAWD77zwS42TNjWCsA5v97ninLD2PIM+HlZIuqwstLD5KRk1cudRFCVE4S3AghSq9+KDh6Q8ZlOLOh3IoNberLhO51AdDrFF7r25iwl3pQy82eyMRMPlp9otzqIoSofCS4EUKUnt766gGbmz64uslfOXitb2M+HNyC5ZO68mSPEFztrfl4WEsAftgZwbbTCYXSq6rKphPxnI5LK7c6CiEqhgQ3Qog70/1FbS+c2MOw7xbbRVj4bCprvY5HOtWhVaCb+V63+l6M7RIEwH9+O1joqIaVBy4xfuEeHl2wBwueFyyEqIQkuBFC3BlHL7jnDe164/uQmT+hNzsV9i2EFZPgy7bwnpf2vYy91rcxQZ4OXErJ5r0/jwEQn5rNW38cBSA6OYsz8ellXg8hRMWR4EYIcefaPwa+zSErCTa8pZ099VU7+PN5bR+cxLOgGuGfT8t86MrBxopPH2qFosDSfRfZcCyO/644QkrW1V6crdcNWQkhqhcJboQQd05vpe1aDFpg88ezkBEPHvXg7v/AyCVg5wYpkXB2Y5lXp32wB0/cVQ+AZ3/Zz4bjcVjrFYa0qQXAv2ckuBGiOpPgRghhGcHdoMVD2rWtK/T5EJ7aBfe+Do3uh1YjtWflMDQF8GKvhjTwcSIrV+speu7eBjyWv8Jq57kr5BotOwdICFF5SHAjhLCcgbNg2AJ4Lhy6PA1WNleftXtU+/PkakiNKfOq2Fnr+fThVthb62lbx40ne4bQ1N8FdwdrMgxGDkQll3kdhBAVQ4IbIYTlWNtB8yHaJOPr+TSGOl20uTf7fyyX6rSs7cbOKffxyxOdsdbr0OkUutbX6nb9UnEhRPUhwY0QovwU9N6Ef69NLI47Bvt/gqSIMivS1cEaWyu9+Xv3guBG5t0IUW1ZVXQFhBA1SNMHYfWr2sTimS0gNVq7b+0Ivd/TVl0pSplWoSC4ORCVTFp2Ls521mVanhCi/EnPjRCi/FjbX51YnBqtnSruEQK5GfDXZPhxCKTFlmkVAj0cCPJ0wGhS2XVODtkUojqS4EYIUb7umQI9XoNh8+E/5+CZvdBnGljZacvElz4KZbyDsAxNCVG9SXAjhChfdq5agNN8KNg6g04HXZ6CJ7aAtQNE7oAjy8q0CgXBzdbTl8u0HCFExZDgRghROfg0hu6Ttev1U8GQUWZFdQ3xQqfA2csZXErOKrNyhBAVQ4IbIUTl0fUZcKujzcfZNrPMinF1sKZ1/oGb/5yS3hshqhsJboQQlYe1PfR+X7ve/iVsnwUrnoT/9dKur5+Lk5tV6vk5dzf0BmCLBDdCVDsS3AghKpcmAyH4LsjLhnWvw8Ff4OJu7fqPZ8GYC7nZ2gnkH9WBOV21XY9LGOQUBDfbziSQJ0cxCFGtyD43QojKRVHggc9h+USw94Da7UHRwZbpsP8HSLqgDVslntPSxx+DX0ZAYCfo/xn4NS9WMa1qu+Fqb01KVi4HLybTLsij7NokhChXEtwIISofrwbwxObC9/xawm+PwYWt2ncnP20IK/4o7JwLUbvgp2Hw7D6wcbxtEXqdQvcGXvx1KIYtJy9LcCNENSLDUkKIqqFxP3j0L6jVHjo9Cc/shpYPQejb8Nx+cAuCtBj494tiZ9mjQf68m2KcM/XXoRhWHojGZCrbPXiEEHdOghshRNVRux1MDIO+07X9cgq4+GvHN4AW3CRHFSu7gnk3hy4mk5hhuGm6f88k8PTP4Ty/+AAjvt3J+YSyW6YuhLhzEtwIIaqHJgMhqLs2EXnD28V6xc/Vjka+zqjqzXcrzskz8ubvR8zfd19I5P6Z//DL7khL1FoIUQYkuBFCVA+KAvd/CChw5DeI3FWs1+5uqO1WvP5YHDl5xhuez/vnHOcSMvB2tmX183fRvb4XOXkmXl9xmMtpOZZsgRDCQiS4EUJUH/6toM1o7XrVi5CTfttXejT0AeDPg5doOnUtoZ9tYcryw+yLSCLySiZfbTwDwBv9m9DE34UfJnSkib8LJhW2n5WzqYSojCS4EUJUL/dNBUcfbRXV8ifAdOs9bDrV86B/S39c7KwwmlTOxKfzy+5Ihs7ZTp+Z/5CTZ6JLPU8GtgoAQFEU7m6g9fb8W8KDNzMNeXy0+gQv/XoQo0xMFqLMyFJwIUT14uQDI36ChQ/Ayb9g47vaiqqbsNbrmP1IW1RVJS41h2MxKaw6FMPqw7Fk5Rqx1iu8N6gZiqKY3+lW34tv/jnHttMJqKpa6BmAqqo88/N+TsSmMqRtbR5qX5uIK5m8vPQgEVcyARjVuQ5t67iXyY9AiJpOUdVS7l1eRaWmpuLq6kpKSgouLi4VXR0hRFk59Ku2ESDA4G+g1YgSvZ6WnUvY8XgC3OzpWLfwHjhZBiOt3lmHwWhi08s9qetVeF+djSfieGzhXvN3K52CUVULbaL8+fBWDG5Tu2RtEqIGK8nvbxmWEkJUTy0fhrte1q7XvKYd2VACznbWDGpT64bABsDeRk+7IK3X5fpVVqqq8kWYNk/n3sY+tK3jRp5JC2webl+b/i38AbiQkFnSFgkhiqlSBDezZ88mODgYOzs7OnXqxO7du2+advny5bRv3x43NzccHR1p3bo1P/zwQznWVghRZdzzX3ANhKwkOLbSoll3L5h3c90GgFtPJ3AwKhk7ax3Th7Zk+VPdWPvC3fz+dDc+HtaK5rW0/XkirsheOUKUlQoPbpYsWcLkyZN56623CA8Pp1WrVvTp04f4+Pgi03t4ePD666+zY8cODh06xPjx4xk/fjxr164t55oLISo9nR7ajdOu935n0ay71deCm+1nE8yTg1VV5cuw0wA80jEIb2dbABr5OdM60A2AYE8HAC5ckZ4bIcpKhQc3n332GRMnTmT8+PE0bdqUuXPn4uDgwPz584tM37NnTwYPHkyTJk0ICQnh+eefp2XLlmzbtq2cay6EqBLajAGdlXb2VOyR26cvpha1XHG2syI1O48j0SkA7Dh3hb0RSdhY6fi/HvWKfC/IU5ufc0F6boQoMxUa3BgMBvbt20doaKj5nk6nIzQ0lB07dtz2fVVVCQsL4+TJk9x9991FpsnJySE1NbXQRwhRgzj7QeP+2vW+BRbLVq9T6BriCWjzbiKvZDLt7xMAjOgQiK+LXZHvBeX33CRn5pKcefMjHwpkGvI4e/n2+/UIIa6q0OAmISEBo9GIr69vofu+vr7Exsbe9L2UlBScnJywsbGhf//+fPXVV/Tq1avItNOmTcPV1dX8CQwMtGgbhBBVQPsJ2p8Hl1zd2C87FfJuH1zcSvf8oan5285z32ebORydgr21nid7hNz0HUdbK/NwVUQxhqae/Xk/9326heMx8h9mQhRXhQ9LlYazszMHDhxgz549fPDBB0yePJnNmzcXmXbKlCmkpKSYP1FRxTtQTwhRjdS9GzzrgyENVr2g7YEzPQi+uRsySr/LcNf84OZKhoFco8rdDb1ZNqkrAW72t65OCYamDl7Uhrx2n08sdT2FqGkqdBM/Ly8v9Ho9cXFxhe7HxcXh5+d30/d0Oh3169cHoHXr1hw/fpxp06bRs2fPG9La2tpia2tr0XoLIaoYRYF242Hd63B46dX7l4/Dj0Ng3J+FTxkvpnpejozsGMjFpCwm9QgxBzu3E+TpwO4LibftucnONZKQrp1fdSI2rcT1E6KmqtCeGxsbG9q1a0dYWJj5nslkIiwsjC5duhQ7H5PJRE6OHGAnhLiFNqOhThcI7AS9P9ACGgcviDkIP48AQ8lXLymKwrQhLflhQqdiBzYAwfmb/l1IuHXPzaXkLPP1yVgZlhKiuCr8+IXJkyczbtw42rdvT8eOHZk5cyYZGRmMHz8egLFjx1KrVi2mTZsGaHNo2rdvT0hICDk5Ofz999/88MMPzJkzpyKbIYSo7Ozd4LE1he+NWa4NUUVuhxkNAAWMBgjsCIO+Brc6ZVKVIPNy8FsHN9GFgps0TCYVnU65xRtCCKgEwc3w4cO5fPkyU6dOJTY2ltatW7NmzRrzJOPIyEh0uqsdTBkZGTz11FNcvHgRe3t7GjduzI8//sjw4cMrqglCiKrKvxU88iv89JA2H6fAha0w9y7t2IZG91u82OD8OTe3G5a6mHQ1uMkwGIlOziLQw8Hi9RGiupGzpYQQIjsV0mK0/XAMGfDn83ApXHvW87/Q81WLFpeWnUuLt9cBcOjt3rjYWReZbsbak8zadMb8fd7Y9vRq6ltkWiGqOzlbSgghSsLOBbwbgWcI+LfUhq86/p/2bPOH2rwcC3K2s8bLyQaAyFv03lw7LAVwQpaDC1EsEtwIIcT1rGyh38fQ4iHt+6ZpFi+iYKfi87eYVHwxSQt8WuSfR3UiTlZMCVEcEtwIIcTN9HgNFB2cWg0X91k064JJxbc6QDM6f87NvY19AG1SsRDi9iS4EUKIm/GqDy1HaNebP7Ro1sHmjfyKHpbKNZqITc0GILSJNs/mfEIG2blGi9ZDiOpIghshhLiVHv/RJhqf2QCRuyyWbcFeNzfruYlNycakgo1eR7MAF9wcrDGaVM7EyzlTQtyOBDdCCHErHnWh9Sjt+q+X4Ngf2oqqOxRs3uum6J6bgmXgAW526HQKjXydARmaEqI4JLgRQojbuftlsHaAuMPw6xj4uB788RwYc0udZZCH1nNzOS2H/ZFJZOTkFXpesFKqtrsWBDX2yw9uZFKxELdV4Zv4CSFEpedWByZuhP0/wvE/ITkCwhdpzwZ8oZ1dVUKuDtZ4ONqQmGFg8NfbAS2AWfJEF1wdrM2TiWvlH8LZ2F/b10POmBLi9qTnRgghisOnCfT5AJ4/CA8tAhQtwPl3ZqmzfPOBJrQLcsfTUdvz5kRsGmuPxQJXl4HXcteCm0b5PTey140Qtyc9N0IIURKKAs0GQdpHsOZV2PA25Gqrmki6oE0+rtUGancAn2agv/k/s4Pb1GZwm9oAfLbuJF9uPMPmk/E83D7wmmEpLbhpmD/nJj4th6QMA+75AZEQ4kYS3AghRGl0fhKSzsOuubDlo8LPDvyo/ekeDGNXan/exr1NfPly4xm2nkog12gyBzcFw1JOtlYEetgTlZjFumOxDO9QNod6ClEdSHAjhBCl1Sd/75v44+AepAUxuVlwcS9E79N6cn4cCo+tA0fPW2bVspYrno42XMkwsOd8IpcKgpv8nhuAB1vVYtamM7y+4gg+Lnbc08injBomRNUmwY0QQpSWTg99pxf9LDUGvusFV87Azw/DuD/AxvHmWekUejTyZnl4NEv2RpFrVNHrFPxc7MxpXuzVkIjETP48eIlJP+7j+8c60bGuR6F8snONJKTnUMvNHqUUE52FqA5kQrEQQpQFF38YvQzs3SF6LywdD6Zb7y5c0BOz+rA2qdjPxQ4r/dV/pvU6hc8ebsU9jbzJzjXx2MI9fBV2msQMA0aTyq97o7hnxma6T99E3y+2snh3pOxoLGokRVVVtaIrUZ5KcmS6EELcschd8P1AyMuG3h9A12dumjQlM5e276/HaNL+We5Y14Nf/6/LDemyDEYeXbCbXecTAbC10uHnakdEERsCejjaMHd0uxt6eISoakry+1t6boQQoizV6QT35084DnsXLp+6aVJXB2va1XE3f6/tZl9kOnsbPT8+3omZw1vTopYrOXkmIq5k4mpvzX/7NWb36/fxRv8m1Ha3JzHDwOOL9nBKNv8TNYj03AghRFlTVW1i8dkwqNVOm2B8kyXiX28+w8drTgLw7L31eal3o9tkrbI3IokLCRn0buqHq4O1+Vl2rpFR/9vFvogk/F3tWP5UV/xdiw6YhKjspOdGCCEqE0WBgV+Brau2imr7FzdNem/jqyugarvfPhBRFIUOwR481D6wUGADYGet57tx7QnxdiQmJZtH5+8hLbv0R0YIUVVIcCOEEOXBtdbVlVVh78GS0RC5U+vVuUYjX2fz3jZ1vZzuuFg3BxsWPdYRH2dbTsal8fv+6DvOU4jKToIbIYQoL61GQLvxgKqdUTW/DyzoC+mXzUkUReHrUW15f1BzOgS73zyvEqjt7sDAVgEARCYWfQq5ENWJBDdCCFFeFAUGzISndkLbsaC3hcgd8MNgyEoyJ2sV6MbozkEW3afGz1XbLyc2NadY6U/GpvHUT/s4LmdZiSpIghshhChvPk20OTiTtoOjD8Qdhh+HQU7ZrWjyzd8MMC4lu1jp3/nzKH8fjuXpn8JlrxxR5UhwI4QQFcWrPoz9/epGfz+PKDrAscCiVv/8npuY1Kzbpj12KZXtZ68AcC4hg5kbTt9x+UKUJwluhBCiIvk2g9HLwcYZIrbB/Psh5aL27MpZbcjq08Zw4d87K6ag5yY1h9vtAPLdtvMA1PPWjouYt/Uchy4m31H5QpQnCW6EEKKi1WoL41bmD1EdgXn3woa3YU5XOLsR0mPhp2Fw/p9SF1EQ3BjyTCRl3nw5eHxqNn8c1FZUffpQKwa0CsBoUvnPb4cw5JlKXb4Q5UmCGyGEqAxqtYOJYeDTFNLjYNvn2pENdXtAvXsgNxN+ehjObipV9jZWOjwdbQCIvcW8mx92RpBrVGkX5E6bOu68PaApHo42nIhN46PVJ0pVthDlTYIbIYSoLNzqwGNrofED4OQHD34NY1fCyMXQoA/kZcEvI+DMhlJlf3XFVNHzbrJzjfy4MwKAx7vXBcDTyZb3BzUHYP6/5/lmy9lSlS1EeZLgRgghKhM7FxjxE7x0AtqM0paPW9vB8B+gUX+tN+eXkXBqbYmz9ssfmopNuWY5uKoSfSmaH3Zc4NEFu0nKzKW2uz29m/mZk/Rr4c+Uvo0BmLb6BL/tu3hnbRSijElwI4QQldH1e9xY2cJDC6HJADAaYPEobSPAEqyk8jX33Fwdltq/5H1qfduUf/9cwM5z2injL4Y2RK8rXP7/9QjhibvrAfDqskP8eyahFI0SonwUfXKbEEKIysfKBoYtgOUT4egK7QgHnTU4eoN7EHR9Fhr1uzEwyud3/V43JhN1Ti8CYKzjTlp1G8u9jX1o5Odc5Puv3d+Yy2k5rNgfzcwNp+hW38v8LNdoYuWBS3Sq60Ggh4MFGy1EyUlwI4QQVYneGob8D2ydIfwHMOVC2iXtE7kDgrrBvW+Aa23QWYGVnbaPjqKY59zEFPTcRGzD06gd/dBROUbXu4NBp79p0Tqdwqv3N2blgWj2XEjifEIGdb205eJfhp3mq41ncLGz4suRbejZyOem+QhR1iS4EUKIqkZvpe1w3PcTyEyAjMtw7A/Y+TVE/KudV3UtWxfwqMddjo1xpydxKVrPTO7+xRScI25lSIWYg9qy9Fvwc7XjrgbebDl1mWX7LvJyn0ZkGvL4foc2ETk1O4/xC/fwSp9GTOoRYtEjJIQoLplzI4QQVZW1ndZDE9AGQt+CZ/dBq5Fg6wpW9tqQFUBOKsQcwP/MYqZbz9Pm3ORmoTv+BwCxeGrpzm8pVrEPta8NwLLwixhNKkv3XiQlK5dgTwdGdgxEVeHjNSeZJkvHRQWRnhshhKguXGvD4LmF7+VmQ9J5iD2M+vskerOP33K2Yziagk1uGhdVLza6DWNsylw4twW6v3jbYkKb+OJqb01MSjb/nL5s3tF4Qve6jOkSTNMAV978/Qjf/nOOexv70LmeZ1m0Voibkp4bIYSozqzttIM6Wz4MXZ8H4G3rRRj3LADgd2M30gK6a2kjd0Le7U8Nt7PW82DrAABeX36YyMRM3B2sGdYuEIAxnYMY2VG7/s9vh8g05Fm6VULckgQ3QghRQyg9XuGS4keAkoh9tHZW1Qpjd9yCWmhHP+RlwcU9xcrrofxA5lL+yqvRnYOwt7k6Gfm//ZpQy82eyMRMpsvwlChnEtwIIURNYW3PQvfnzF+PKyGcVWvR0M8F6t6t3TxXvHk3zWu50Dh/ybiNXsfYLsGFnjvbWTN9aEsAFu2IYPvZG/fFibySSa5RzqsSlifBjRBC1CCXfbuxzKgNQy003AtAQx/nq8HNtYdz3mKDQEVRGNMlCIARHQPxdra9IU33Bl480qkOAK8sPURK1tUDO5fsieTuTzYxbM52sgzGO2qTENeT4EYIIWoQXxc7/pP7f7zg9hVLjD3xcbbF1cEa6vXQEkTvhYt7Yel4+MAf/noZctKLzOuRjnX445luvPlA05uW999+TQjydCA6OYs3fj+CqqociU7hzZVHATh4MYX/LDuEWoKdloW4HQluhBCiBvF3tcOInpVxnoBCQ9/83Yjdg8EtCEx58L/74OhybQ7Onnkwpwuc23xDXoqi0LK2G9b6m/8qccpLZoXf99ytP8yfBy/xw84Inv45HEOeiVaBbljpFP48eInZm86USXtFzSTBjRBC1CC++UcwFHSUNPB1uvow5N6r100fhEFzwDUQkiPh+wfhzxcgO7VkBW77HI+zy/mf3RcEKzFMXXmUiCuZ1HKzZ9H4Drz7oHbi+Ix1p1h3NPYOWibEVRLcCCFEDVJwBEOBRr7XnCN131To/T5M2gEPfw+tH4GndkD7CdrzfQvg6y5wZkPxCjNkwv4fAbAxZvKd4xxsyMVarzB7VFvcHGx4pFMdxuXP3flozQkZnhIWIcGNEELUIAWHZxZocG1w4+ChHb7pe80cGltneOAzGPenNmyVehF+HAo7r9sssChHlkF2MrjUBnt3QvLOMNtnJZ8+3JrWgW7mZC/3aYSNXse5yxmcib86vycpw8CwOdv5bN3JmxZhNKlk58qEZFGYBDdCCFGDeDvbotddPe+p0LDUrdS9W+vF6fC49n39VEi4xTwZVdXm6wB0nKgNcQG9Upcz0O5goaTOdtZ0q6/tYrz2mqGpn3dHsjciiS83niHseJz5flRiJv2/3Er9//5NyH//pvGba5j0477itUPUCBLcCCFEDaLXKXg7acu2/V3tcLGzvs0b17BxhH4ztLk5xhz44xkw3WSfmuh92kGceltoMwYa9YXOT2nP1k+9YZn5/c39AFiTH9yoqsrSvVHm568tP0xShoGUzFweXbCbo5dSyTNdzWPN0ViSMw3Fb4uo1iS4EUKIGsY3f95NoSGp4lIUeGAmWDtC5A7Y+x3kpMHWT+Hbe+Cvl7QJyHv+p6VvPgQc88+W6jkFbJwg4RRc2Foo29AmvugUOBKdysWkTHadT+TClUwcbfSEeDtyOS2H138/zJM/7uPs5Qz8XOxY9+Ld7H+zF/W8HFFV2Hsh6Q5+KqI6keBGCCFqGD8XreemoU8xh6Su5x4EoW9r1+vfgpktIOxduBSuBTVftoHDS7XnBcNYAHYu0HK4dl0Q/OTzdLKlQ7AHAOuOXOLXPVqvzcDWAXw+vDV6ncLfh2PZce4KjjZ65j/agYa+zrg72tCxrvbenguJpWuPqHYkuBFCiBrmgZYB1HKzp19L/9Jn0uFxqNMFcjMgKwk8QqDPNKjXU9srx5QH/q2gVrvr3stfeXV8FaTGFHrUp5kfDZUoHtrYgw5H3wPg4faBtKztxtP31Ae0YbXZo9rSNMDF/F5BcLPrvAQ3QmNV0RUQQghRvga0CmBAq4A7y0SngyHz4J+PIagbNB8Geivo8pS2w/HRFdBmtDaMdS3fZlpQFLkDwr+Hnq+aH/Vp7kfw2l9wVtMZqGzle5+nzKuqnr23Ps62VjT2d+auBt6FsiwIbo5Ep5BpyMPBRn611XTyvwAhhBCl4xYIA7+68X7t9trnZjo8rgU3+xbAXZNBr01qrpVygFr6AwA4Kjk82TAdJT84stbrmHh3vSKzq+3uQICrHZdSstkfmUy3+l531CxR9VWKYanZs2cTHByMnZ0dnTp1Yvfu3TdNO2/ePO666y7c3d1xd3cnNDT0lumFEEJUMk0GgKM3pMXAydXaPVXV5u1co5dD8Y9kkKEpca0KD26WLFnC5MmTeeuttwgPD6dVq1b06dOH+Pj4ItNv3ryZkSNHsmnTJnbs2EFgYCC9e/cmOjq6nGsuhBCiVKxsoe1Y7XrVi3BkubbrceR2VL0tv1oPAsAhZmexs+yQH9zsPn/F0rUVVZCiVvBe1506daJDhw7MmjULAJPJRGBgIM8++yyvvfbabd83Go24u7sza9Ysxo4de9v0qampuLq6kpKSgouLy23TCyGEKAMZV2DRAIjXTgfH2lGbnNzlGWjxEHzbA2xd4NULoNPfNrsz8WmEfvYPtlY6Dr/dBxurCv9vd2FhJfn9XaF/+waDgX379hEaGmq+p9PpCA0NZceOHcXKIzMzk9zcXDw8PIp8npOTQ2pqaqGPEEKICuboCU9shh6vgc5aC2xsnKH7ZPBrAbaukJMKsYeKlV2ItxMejjbk5Jk4HJ1cplUXlV+FBjcJCQkYjUZ8fX0L3ff19SU2tninw7766qsEBAQUCpCuNW3aNFxdXc2fwMDAO663EEIIC7CygXumwP9tgZYjYMg3WtCj00OdzlqaC/8WKytFUegQ7A7IvBtRCebc3ImPPvqIxYsXs2LFCuzs7IpMM2XKFFJSUsyfqKioItMJIYSoIL7NtMCmcf+r94K7aX9G5Ac3SREwuzN81xv2/6SdOH6djnW1nZA3nYjnclpOWddaVGIVuhTcy8sLvV5PXFxcoftxcXH4+fnd8t0ZM2bw0UcfsWHDBlq2bHnTdLa2ttja2lqkvkIIIcpJUHftz4jtkGeAZY/D5ePavahdsGYK9HhFO8U8XyfzTsVJdPhgA80CXBjXNZiH20uPfU1ToT03NjY2tGvXjrCwMPM9k8lEWFgYXbp0uel7H3/8Me+99x5r1qyhfftb7KUghBCiavJvpZ1DlZ0Myx6Di7u1CcY9XgW3IMhJgXVvwL5F5leaBbgwpW9jmuXvXnz0UiqvrzhMWnZuBTVCVJQKH5aaPHky8+bNY9GiRRw/fpxJkyaRkZHB+PHjARg7dixTpkwxp58+fTpvvvkm8+fPJzg4mNjYWGJjY0lPT6+oJgghhLA0vRUEdtKuj/+p/fnA53DPf+G5A3DXy9q9vybDuS2ANu/m/3qE8Ndzd7Hn9VDqeDiQa1TZcVaWh9c0FR7cDB8+nBkzZjB16lRat27NgQMHWLNmjXmScWRkJDExV88fmTNnDgaDgWHDhuHv72/+zJgxo6KaIIQQoiwUzLsBaD0aWgzTrnU6uPcNaD5UO8Pq1zGQcLrQq97OttzTSDumYfOpy3B+q3YshAX9c+oyP+2KoIJ3VBFFqPB9bsqb7HMjhBBVROwR+OYu7VDOJzaD7XWnmOdmw6IH4OIe8KgHj4eBw9VtQTaeiOOxhXu52yWORYbJKDorePQvqNPpjqt2+GIKQ+b8S65R5aMhLRjRsc4d5ylurcrscyOEEELclF9zmLQdJm68MbABsLaDET+Dax1IPAdLxmiTj/N1rueJjV7Hw1mLUVDBlAu/jiExNoLkTMON+RVTek4ez/4STq5R6xv44K/jxKRklTo/YXkS3AghhKi8fJqA3S3+K93JBx5Zom0AGLENVr2gnVMFONhYMbh2Kv10+ecPugdDehxRc4cx6ItNGPJMxarCxhNxfPvPWS4lawHMWyuPcuFKJgGudrSq7UpaTh7/XX5YhqcqEQluhBBCVG2+TeGhhaDo4MBPsOVjc4AzkeXoFJU99t1h9HIydU604hSvZM7gyIHrzq7KzQZj4ZVVl5KzeOL7fXz49wm6T9/I0DnbWRZ+EZ0CM0e04dOHW2Gj17Hp5GWWh8sZh5WFBDdCCCGqvgah0Pdj7Xrzh7BsAkSHExK3FoD30x5gT5o7T2U/hUlV6K/fTdtVfeGbu2HpozCrA3zoD1+1hfTL5mznbztPnknF2dYKkwr7IpIAePbeBnSs60F9H2eeD20AwDt/HmW37I5cKciEYiGEENXHjq9h/ZvaKipFD6qRrboOjMl8EU9HG65kGBjkepr+mX9wj/4AVhhvzKNBH3hkCSnZeXSdFkaGwciCRzsQ5OnAb/suoijwYmhDrPRa/0Ce0cSYOWFEXLxEnM6byb0aMqlHCDqdUs6Nr95K8vu7QncoFkIIISyqy1NQqy0sHQ9plwA4WO8JOAJXMgzYWet4ZsIE+n3RCKfcZNb0TsLHxgC+zbWzrn4cBqfXwq5v+Ck7lAyDkUa+zvRs5I2iKPzn/sY3FGmVEctPuZMx2sfSK3san6xV2X0+kdmj2uJkK79mK4IMSwkhhKhe6nSGJ7dCu/EQ+g71W99lfvTE3SHU93Gmc4gnibiwQt8Hur+gDWvVvRt6vw+Auv5Ntm3bnP9OPRTlJr0wWcnw4zB0KZFYqwa+bnYSWysdW05d5rEFe8g05JVtW0WRJLgRQghR/Th6wYCZ0P0FutX3wtvZlrpejjzZox4AoU18AAg7EV/4vY4ToWFfFKOBL3LfYY7DNzyYvRIun7qxjNxsWDwK4o+C3gaApglrWPJEJ5xtrdh9IZHHFu4hy1DE0JcoUxLcCCGEqNac7azZ/HJP/nquOw422jDRPY204GZfRFLhPW8Uhax+XxKj+OCtpNLXtAWr9f+FOV3gxN9X0+XlaJOWI7ZpZ149+re2HD05ktbqSRZN6IiTrRU7zyXy+Pd7yDUWb9m5sAwJboQQQlR7jrZW5sAGINDDgUa+zhhNKltOXV0dpaoqr/x9kfuyPuI53X/Jues1qNNFm6C8dByc3QjZKfDjUDixSuuxGfETBHaApgO1TA4toW0ddxY91gFHGz3/nrnCHwculXeTazQJboQQQtRI9xUMTR2/OjT19eazrDoUg0Fnz+gxE7G9bwqMWwVNBoLRAL88At/1hgtbtZ6aUb9pc3UAWg7X/jy6AvJyaBfkwZM9QgBYvCeyXNtW00lwI4QQokYqCG7+PhzDmO928f6qY8xYdxKAdx5sRse6+edU6a1g6HfQoDfkZcHlE+DoDY+ugno9rmYY3B2cAyA7GU6vA2B4cyf66vfQ/eK3pC8YBj89XGgfnWJLOA2n1pk3JxS3JvvcCCGEqJGMJpVhc7ezPzK50P3Rnevw/qAWN76QmwW/T4LkKBjyLXiG3Jhm3Zuw/Uuo1xO8m0D4IsjNLJymfig8slQ73fxWslNg7wI4sgxiD2n3er8PXZ8tMnlOnpEdZ6/Qvb6XeQ+e6qQkv78luBFCCFFjqarK6fh0tp5OYPuZBLydbXlvUHOsSxscxB6Bud0K3cpwqceqxDrEWgXwnNVylLxs6P0BdH3m5vkY82DB/dqJ59eyd4fnD4Kd69V7Wclg78Zryw6xeE8Ubw9oyqPd6pau/pWYnAouhBBCFIOiKDT0dWZC97p892gHPhrasvSBDWgnmdfpql3X7QFjVmD3/D6+dHqez7Mf4EDT/2jPNrwNl/bfPJ+tM7TAxtYVHpgJL58G78aQlQTbv9LSqCqsfAamB5Ox9j3z2Vbbz14peb0vHYCvuxZeEVaFSXAjhBBCWNKopfDCERj3B4Tci16vY3iHQADei+nE5cA+YMol7fuRmNa9Bbu+gdMbtN4agIt7tcM/Afp/Cu3Ha6ef3/uGdm/H15AeD1umw/4fABXHHTOYxFIADl1MKXmdN3+k7dez4e3C83pys7X6JUeV7mdRQSS4EUIIISzJ1gncAgvderh9IHqdQnhUCqGnhxKteuKcHYNu+0xY/R/4aSh80Qq2zYTlT4BqhOZDoeVDVzNp/ADUage5GfDzw7B5GgDGhv0BeNF6Gc/oVxCbmk1canbx65t6STtyAiDhZOGhsH8+0er3y4irwdftRO7SepgqkAQ3QgghRBnzc7VjVKc62Oh1eHn78obnTD7MHclSq/6YGg8ABy9IvQgb3oLEs9qqq/6fFs5EUeC+qdp1wZBWtxdY3nA603JHAvCy9VI+sZrL0bMRxa/cgZ9AvWaTwfDvtT+zU2H3PO067gjs/vbW+agq7PoWFvaDZRPBVHE7M0twI4QQQpSDdx9szsn37yfspZ58/WR/ltkN5ZX0UfzRaDq8eBQenK0d4GntAEO+0SYP58syGDkSnaKtwqrXU7vZdBDqfVOZ/+8FvjEOYGe95zCh8JDVP3T6+344tvL2lTKZIPwH7brtWO3PI8shJw32zoecFLCy1+5v+hDSYovOJzcbVj4Nq1/RNjy0cwVjbql+TpYgwY0QQghRTgoO4LS30TO+WzAAczafxaS3hTajYdK/MOXi1Y0B801bfZwHvtrGxhNxMHS+tu/OkG/ZeT6Z4zGp2FvraTJsKus6LeKMKQDH3ET4dax2ynnCmZtX6PxmSI7QJi7fPx08QrRhr4OLYefXWpp+n2jDYYY0WPfGjXlkJsKCvloPkKLTlqsP/R9Y21ngJ1Y6EtwIIYQQFWBMl2CcbK04GZdW+ABPnf6GtBvzn++5kASOntBiGFjZsmL/RQCGtK2Fq4M1tVr0pJ9hGt8wFFVnDWfWw9ed4a+X4c/n4Zu74eMQ7Xtm4tUhqJYPg40DtB2jfV/3BqTHgUttbefl/p8CChxeCuf/ITvXyOxNZzgRmwprXoNL4VpP0+jl2j48NztFvZxIcCOEEEJUAFd7a0Z1rgPA15vPcLNt5xLSc7iYlAVAxJWMQs9Ox6cD0K2+FwCN/JxBb8u07KFEj9wI9XuBKRf2zIN9CyHmIGQmaN+/agvHV2kZFQxJtXoEFD3k5U9I7voMWNlAQBvoMEG7t+xxFv79D5+sPckfv86HQ0u0HptRyyDkHgv9dO6MBDdCCCFEBZnQvS42Vjr2RyYXOuPqWgeu2UH5fELh3Y7PJ2jBTl0vRwBsrHQ0DdA2uNuX4Qmjf4ORS6DFQ9DtBXhoEYxcDD5NtRVNplwtcPFvqWXo7AsN+2jX9h5Xgx6A+94Cn2aQHkdo+NPUVi4z7spM7VmXZ6B2uzv6WViSBDdCCCFEBfFxtmNclyAAXl12iPgilnAfiEo2X0dcyTD38CRlGEjO1CbtBns6mtO0qq3tXmze76bR/docmF7vQLNB0Kgv/N9WbaipTldtt+Rr3fWStlqr17tgczVf7Fxg1FJSrb2pr0Sz1uY/+CpJGFzrwj3/vcOfhGVJcCOEEEJUoJd6N6KxnzNXMgxM/vUgJlPh4alrg5tMg5HLaTkAnMvvtQlwtcPe5uo8nZa13QA4eM17N9BbQYfH4bHVEFz4uAhqt4eXjl+df3ONOMWTUVmvkKra46jkYFIVdrZ4B6zti9/gciDBjRBCCFGB7Kz1zHqkDXbWOradSeDbrefMz0wm1RykWOm0SboFQ1HmISlvx0L5tQp0A+DIpRTyjCYsafamMxzOq81nHlNJsvFnRt5DbM5qYNEyLEGCGyGEEKKC1fdx5u0BzQCYsfYkx2NSATiXkE5aTh521jo61fMA4MKVguBGm0xcMN+mQD0vR5xtrcjONZknHBflQkIGP+6M4Ep6TrHqGJ2cxS+7IwHo88BwNvZez9fGQRyOTi5+Q8uJBDdCCCFEJTC8QyC9m/qSZ1KZs/ksAPvzJxO3rOVGiLcTABeuaJOKr04mdiqUj06n0LyWNu9m3j/niEq8Ogk5McPAHwcvMep/O+k5YzNv/H6EEd/uJCnDcNv6rQi/SK5RpVNdD7qEeNKyoIcoOhWjqeiVXhXFqqIrIIQQQghtg7/nQxuw7lgcfx2O4dW+jc3zbVrXccPPRdsU70J+UHPusvZnvet6bgC6N/Bix7krLN8fzYoD0XQI8iA2NZvIawIdRQFHGytOx6czfuEefnq8E462Nw8LNp+8DMCAVgFaud5OONjoyTQYOXc5nQa+znf+Q7AQ6bkRQgghKolmAa50DfHEaFJZ+O/5q8FNoBvBXg6A1mNjMqnm4anrh6UAJvUIYc6ottzVwAtVhd0XEs2BTT1vR567tz5b/3MPK57qipuDNQeikvm/H/aRk1f0eVApmbmER2qHYfZs5A2AXqfQPOC6lVnAydi0m+7ZU16k50YIIYSoRB6/qy7bz17hl91RZOVqwUbrQDey868jrmQSk5pNdq4Ja71CbfcbVyrpdAp9W/jTt4U/FxIy+PdsAkEejrSo7YqrvXWhtAvHd+SReTvZdiaB2ZvOMrlXwxvy23rmMiYV6vs4UdvdwXy/eS1Xdl9I5HB0CkPb1eZ4TCr9v9xKt/pe/G9ce2ytbtxtuTxIz40QQghRifRs6EOItyPpOXkYTSo+zrb4u9pR290BnQJZuUZ2nbsCQB0PB6z0t/5VHuzlyKhOQXRv4HVDYANa4PRG/6YA/Hsmocg8Coakejb0LnS/pXlPnWRUVeXDv49jUsHFzrrCAhuQ4EYIIYSoVHQ6hcfvqmf+3jrQDUVRsLHSmXtNNuUHG9dPJi6tjnW1E8iPFrF83GRS2XIqP7hp5FPoWYv84OZYTCphx+PZejoBG72OV+9vbJF6lZYEN0IIIUQlM7hNLTwdbQBtMnGB4Pz5NVtOakc11PO+cb5NadT1csLRRk92romzlwufX3UsJpXLaTk42OjpkB8Emd/zdMQpf9n5f5YdAuDRbsHU8XSgIklwI4QQQlQydtZ6PhjcnO71vRjWtrb5fnB+0JCanQcUPZm4NPQ6hWb5k4MPR6cUelbQa9M1xPOGoSadTqFZ/llWiRkG3B2sefqe+hap052Q4EYIIYSohO5v7s+Pj3fCJ38JOBQ+QwosF9wA5r1xDl9MLnR/c34vUY/rhqQKFMy7AXixV8Mi5/WUNwluhBBCiCqiYDl4gaL2uCmtgiDl2p6blKxcwvM3Erx+MnGB9sHazskh3o6M7FjHYvW5E7IUXAghhKgiru25cbTR4+1sa7G8C3pujsWkkmc0YaXX8c+pyxhNKvV9nAj0KHoeTe+mvnwxojUd63pgfZuVW+WlctRCCCGEELdVsBwctAMzFUWxWN71vBzNk4rPXNbOpFqxPxqAXk19b/qeoig82LoW/q6V52RwCW6EEEKIKuLa5eCWWgZeQKdTaGaed5NCfGq2eb7NsHa1b/VqpSPBjRBCCFGFBHkWBDeWm29ToEV+cHMkOoXl+6MxqdC2ztVDO6sKmXMjhBBCVCEPtQ8kOjmLvs39LJ53QXBzKDqFbfm7FT/UPtDi5ZQ1CW6EEEKIKmRgqwAG5p/MbWkFOw4fiEpGVcHOWscDLf3LpKyyJMNSQgghhACu7jhccKh3v+b+ONtV/L41JSXBjRBCCCGAwjsOAwxrX7UmEheQ4EYIIYQQZgXzbmq729O5rmcF16Z0JLgRQgghhNnQdrUJ8nTglT6N0Okst49OeZIJxUIIIYQwa+LvwpZX7qnoatwR6bkRQgghRLUiwY0QQgghqpUKD25mz55NcHAwdnZ2dOrUid27d9807dGjRxk6dCjBwcEoisLMmTPLr6JCCCGEqBIqNLhZsmQJkydP5q233iI8PJxWrVrRp08f4uPji0yfmZlJvXr1+Oijj/Dzs/zOjEIIIYSo+io0uPnss8+YOHEi48ePp2nTpsydOxcHBwfmz59fZPoOHTrwySefMGLECGxtLXfMuxBCCCGqjwoLbgwGA/v27SM0NPRqZXQ6QkND2bFjR0VVSwghhBBVXIUtBU9ISMBoNOLr61vovq+vLydOnLBYOTk5OeTk5Ji/p6amWixvIYQQQlQ+FT6huKxNmzYNV1dX8ycwsOqdbiqEEEKI4quw4MbLywu9Xk9cXFyh+3FxcRadLDxlyhRSUlLMn6ioKIvlLYQQQojKp8KCGxsbG9q1a0dYWJj5nslkIiwsjC5dulisHFtbW1xcXAp9hBBCCFF9VejxC5MnT2bcuHG0b9+ejh07MnPmTDIyMhg/fjwAY8eOpVatWkybNg3QJiEfO3bMfB0dHc2BAwdwcnKifv36FdYOIYQQQlQeFRrcDB8+nMuXLzN16lRiY2Np3bo1a9asMU8yjoyMRKe72rl06dIl2rRpY/4+Y8YMZsyYQY8ePdi8eXN5V18IIYQQlZCiqqpa0ZUoT6mpqbi6upKSkiJDVEIIIUQVUZLf39V+tZQQQgghapYKHZaqCAUdVbLfjRBCCFF1FPzeLs6AU40LbtLS0gBkvxshhBCiCkpLS8PV1fWWaWrcnBuTycSlS5dwdnZGURSL5p2amkpgYCBRUVE1Yj5PTWsv1Lw217T2Qs1rc01rL9S8NleX9qqqSlpaGgEBAYUWGxWlxvXc6HQ6ateuXaZl1LT9dGpae6HmtbmmtRdqXptrWnuh5rW5OrT3dj02BWRCsRBCCCGqFQluhBBCCFGtSHBjQba2trz11lvY2tpWdFXKRU1rL9S8Nte09kLNa3NNay/UvDbXtPZCDZxQLIQQQojqTXpuhBBCCFGtSHAjhBBCiGpFghshhBBCVCsS3AghhBCiWpHgxkJmz55NcHAwdnZ2dOrUid27d1d0lSxm2rRpdOjQAWdnZ3x8fBg0aBAnT54slCY7O5unn34aT09PnJycGDp0KHFxcRVUY8v66KOPUBSFF154wXyvOrY3Ojqa0aNH4+npib29PS1atGDv3r3m56qqMnXqVPz9/bG3tyc0NJTTp09XYI1Lz2g08uabb1K3bl3s7e0JCQnhvffeK3RmTVVv7z///MOAAQMICAhAURR+//33Qs+L077ExERGjRqFi4sLbm5uTJgwgfT09HJsRfHdqr25ubm8+uqrtGjRAkdHRwICAhg7diyXLl0qlEdVai/c/u/4Wk8++SSKojBz5sxC96tam4tLghsLWLJkCZMnT+att94iPDycVq1a0adPH+Lj4yu6ahaxZcsWnn76aXbu3Mn69evJzc2ld+/eZGRkmNO8+OKL/PnnnyxdupQtW7Zw6dIlhgwZUoG1tow9e/bwzTff0LJly0L3q1t7k5KS6NatG9bW1qxevZpjx47x6aef4u7ubk7z8ccf8+WXXzJ37lx27dqFo6Mjffr0ITs7uwJrXjrTp09nzpw5zJo1i+PHjzN9+nQ+/vhjvvrqK3Oaqt7ejIwMWrVqxezZs4t8Xpz2jRo1iqNHj7J+/XpWrVrFP//8wxNPPFFeTSiRW7U3MzOT8PBw3nzzTcLDw1m+fDknT55k4MCBhdJVpfbC7f+OC6xYsYKdO3cSEBBww7Oq1uZiU8Ud69ixo/r000+bvxuNRjUgIECdNm1aBdaq7MTHx6uAumXLFlVVVTU5OVm1trZWly5dak5z/PhxFVB37NhRUdW8Y2lpaWqDBg3U9evXqz169FCff/55VVWrZ3tfffVVtXv37jd9bjKZVD8/P/WTTz4x30tOTlZtbW3VX375pTyqaFH9+/dXH3vssUL3hgwZoo4aNUpV1erXXkBdsWKF+Xtx2nfs2DEVUPfs2WNOs3r1alVRFDU6Orrc6l4a17e3KLt371YBNSIiQlXVqt1eVb15my9evKjWqlVLPXLkiBoUFKR+/vnn5mdVvc23Ij03d8hgMLBv3z5CQ0PN93Q6HaGhoezYsaMCa1Z2UlJSAPDw8ABg37595ObmFvoZNG7cmDp16lTpn8HTTz9N//79C7ULqmd7//jjD9q3b89DDz2Ej48Pbdq0Yd68eebn58+fJzY2tlCbXV1d6dSpU5Vsc9euXQkLC+PUqVMAHDx4kG3bttG3b1+g+rX3esVp344dO3Bzc6N9+/bmNKGhoeh0Onbt2lXudba0lJQUFEXBzc0NqJ7tNZlMjBkzhldeeYVmzZrd8Lw6trlAjTs409ISEhIwGo34+voWuu/r68uJEycqqFZlx2Qy8cILL9CtWzeaN28OQGxsLDY2NuZ/JAr4+voSGxtbAbW8c4sXLyY8PJw9e/bc8Kw6tvfcuXPMmTOHyZMn89///pc9e/bw3HPPYWNjw7hx48ztKup/51Wxza+99hqpqak0btwYvV6P0Wjkgw8+YNSoUQDVrr3XK077YmNj8fHxKfTcysoKDw+PKv8zyM7O5tVXX2XkyJHmgySrY3unT5+OlZUVzz33XJHPq2ObC0hwI0rk6aef5siRI2zbtq2iq1JmoqKieP7551m/fj12dnYVXZ1yYTKZaN++PR9++CEAbdq04ciRI8ydO5dx48ZVcO0s79dff+Wnn37i559/plmzZhw4cIAXXniBgICAatlecVVubi4PP/wwqqoyZ86ciq5Omdm3bx9ffPEF4eHhKIpS0dUpdzIsdYe8vLzQ6/U3rJSJi4vDz8+vgmpVNp555hlWrVrFpk2bqF27tvm+n58fBoOB5OTkQumr6s9g3759xMfH07ZtW6ysrLCysmLLli18+eWXWFlZ4evrW63aC+Dv70/Tpk0L3WvSpAmRkZEA5nZVl/+dv/LKK7z22muMGDGCFi1aMGbMGF588UWmTZsGVL/2Xq847fPz87thUUReXh6JiYlV9mdQENhERESwfv16c68NVL/2bt26lfj4eOrUqWP+dywiIoKXXnqJ4OBgoPq1+VoS3NwhGxsb2rVrR1hYmPmeyWQiLCyMLl26VGDNLEdVVZ555hlWrFjBxo0bqVu3bqHn7dq1w9rautDP4OTJk0RGRlbJn8F9993H4cOHOXDggPnTvn17Ro0aZb6uTu0F6Nat2w3L+0+dOkVQUBAAdevWxc/Pr1CbU1NT2bVrV5Vsc2ZmJjpd4X/+9Ho9JpMJqH7tvV5x2telSxeSk5PZt2+fOc3GjRsxmUx06tSp3Ot8pwoCm9OnT7NhwwY8PT0LPa9u7R0zZgyHDh0q9O9YQEAAr7zyCmvXrgWqX5sLqegZzdXB4sWLVVtbW3XhwoXqsWPH1CeeeEJ1c3NTY2NjK7pqFjFp0iTV1dVV3bx5sxoTE2P+ZGZmmtM8+eSTap06ddSNGzeqe/fuVbt06aJ26dKlAmttWdeullLV6tfe3bt3q1ZWVuoHH3yg/n979xLaxBqGcfxJvcQkWowGa1SKFkutiiJ4oehGuzB1oZaKWIJEN6VWSzfiphbrQnBVFy4CBe3GolDxUhUVlG4sVF2kl0UtLrrT4m1hUmpd5D0LIZzx2nNO7Zg5/x8MZOabTN+XkPRh5pvk5cuX1tnZacFg0K5cuZLb5/z587Zw4UK7ffu2DQ4O2r59+2zVqlU2MTHhYuX/TiKRsOXLl9vdu3dtdHTUbty4YZFIxE6dOpXbJ9/7TafTlkqlLJVKmSRra2uzVCqVuztoKv3FYjHbtGmTPX361J48eWKlpaVWW1vrVks/9bN+P3/+bHv37rUVK1ZYf3+/43NscnIyd4x86tfs16/x176+W8os/3qeKsLNNLl48aIVFxfb3LlzbevWrdbX1+d2SdNG0neXjo6O3D4TExPW0NBg4XDYgsGgVVdX2+vXr90repp9HW682O+dO3ds/fr15vf7bc2aNdbe3u4Yz2az1tLSYkVFReb3+62ystJGRkZcqva/+fjxozU1NVlxcbHNmzfPSkpKrLm52fGPLt/77enp+e77NpFImNnU+nv//r3V1tba/PnzrbCw0I4ePWrpdNqFbn7tZ/2Ojo7+8HOsp6cnd4x86tfs16/x174XbvKt56nymf3tKzkBAADyHHNuAACApxBuAACApxBuAACApxBuAACApxBuAACApxBuAACApxBuAACApxBuAPzv+Xw+3bp1y+0yAEwTwg0AVx05ckQ+n++bJRaLuV0agDw12+0CACAWi6mjo8Oxze/3u1QNgHzHmRsArvP7/Vq6dKljCYfDkr5cMkomk6qqqlIgEFBJSYmuX7/ueP7Q0JB27dqlQCCgxYsXq66uTplMxrHP5cuXtW7dOvn9fkWjUZ04ccIx/u7dO1VXVysYDKq0tFTd3d2/t2kAvw3hBsAfr6WlRTU1NRoYGFA8HtehQ4c0PDwsSRofH9fu3bsVDof1/PlzdXV16dGjR47wkkwmdfz4cdXV1WloaEjd3d1avXq142+cPXtWBw8e1ODgoPbs2aN4PK4PHz7MaJ8Aponbv9wJ4P8tkUjYrFmzLBQKOZZz586Z2Zdfpa+vr3c8Z9u2bXbs2DEzM2tvb7dwOGyZTCY3fu/ePSsoKLCxsTEzM1u2bJk1Nzf/sAZJdvr06dx6JpMxSXb//v1p6xPAzGHODQDX7dy5U8lk0rFt0aJFuccVFRWOsYqKCvX390uShoeHtXHjRoVCodz49u3blc1mNTIyIp/Pp1evXqmysvKnNWzYsCH3OBQKqbCwUG/evPm3LQFwEeEGgOtCodA3l4mmSyAQmNJ+c+bMcaz7fD5ls9nfURKA34w5NwD+eH19fd+sl5eXS5LKy8s1MDCg8fHx3Hhvb68KCgpUVlamBQsWaOXKlXr8+PGM1gzAPZy5AeC6yclJjY2NObbNnj1bkUhEktTV1aXNmzdrx44d6uzs1LNnz3Tp0iVJUjwe15kzZ5RIJNTa2qq3b9+qsbFRhw8fVlFRkSSptbVV9fX1WrJkiaqqqpROp9Xb26vGxsaZbRTAjCDcAHDdgwcPFI1GHdvKysr04sULSV/uZLp27ZoaGhoUjUZ19epVrV27VpIUDAb18OFDNTU1acuWLQoGg6qpqVFbW1vuWIlEQp8+fdKFCxd08uRJRSIRHThwYOYaBDCjfGZmbhcBAD/i8/l08+ZN7d+/3+1SAOQJ5twAAABPIdwAAABPYc4NgD8aV84B/FOcuQEAAJ5CuAEAAJ5CuAEAAJ5CuAEAAJ5CuAEAAJ5CuAEAAJ5CuAEAAJ5CuAEAAJ5CuAEAAJ7yF8wwSRggnE46AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Which approach(es) did you find helpful to improve your model performance?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your answer here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
